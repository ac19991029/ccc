期刊：Association for Computational Linguistics

# Multi-Class Classification using BERT models for Detecting Depression Signs from Social Media Text

# 利用BERT模型的多分类方法从社交媒体文本中检测抑郁迹象

## abstract

本文旨在通过人们在社交媒体上分享自己的感受和情绪的信息和帖子来确定一个人的抑郁迹象。鉴于社交媒体上的帖子都是英文的，该系统应该将抑郁症的症状分为三种标签，即“不抑郁”、“中度抑郁”和“严重抑郁”。为了实现这一目标，我们采用了一种经过微调的BERT模型。实验在测试集上达到了58.8%的准确性。

## introduce

抑郁症是一种常见的精神疾病，患者必须主动接触才能诊断并治疗。研究表明，抑郁症是可以预防的，可以在早期发现，社交媒体作为人类最重要的公共交流平台之一，为精神疾病的早期识别和管理提供了潜在的前景（Priyadharshini等人，2021年;Kumaresan等，2021）

由社交媒体(Chakravarthi, 2020;Chakravarthi and Muralidaran,，2021年)。在社交媒体上，许多多媒体内容，主要是简短的文字和照片，不断交换(Chakravarthi等人，2021,2020)。与传统的人际接触不同，互联网的信息可能会被熟人迅速传播，并被陌生人访问(Sampath等人，2022年;Ravikiran等人，2022年;Chakravarth等人，2022年;Bharathi等人，2022年;Priyadharshini等人，2022年)。这种方法避免了用户与个人的直接互动，同时也增加了他们表达情感的冲动。

第二届语言技术促进平等、多样性和包容性研讨会(LT-EDI)的任务4旨在从英语文本中检测抑郁(Durairaj等人，2022年)。本研究证明了BERT Transformer模型可以有效地将社交媒体文本分为“非抑郁”、“中度抑郁”和“重度抑郁”3类。

该模型使用各种来源的社交媒体文本进行训练，过程包括两个子任务：预处理和训练。在subtask-a中，将文本进行清理，转化为更适合上下文和情绪分析的格式，用于抑郁检测，在subtask-b中，对任务数据训练了一个简单的bert分类模型，并对其性能进行评价。

## background

transformers 每个输出元素都与输入元素相关，它们之间的权重根据它们之间的关系动态确定，在NLP中，被称为注意力

BERT基于transformers，代表transformers的双向编码器表示，早期模型只能从右到左或从左到右线性读取文本，不能够同时做，BERT被设计成同时在两个方向上阅读。

## related work

### 抑郁检测

为了使早期干预可行，检测抑郁症的模型必须非常精确和迅速。(Shen et al.， 2017)主张提取6个特征组，然后使用这些特征组训练一个多模态抑郁字典学习模型来检测抑郁的Twitter用户。(Burdisso et al.， 2019)提出了用于社交媒体流早期抑郁症诊断的SS3文本分类系统，该系统简单有效。(Lin et al.， 2020)提出了SenseMood，这是一个使用BERT分类器和CNN对抑郁/不抑郁的社交媒体信息和照片进行分类的系统。(?)认为现有的抑郁检测评估在量化模型延迟方面是无效的，并提出了解决这个问题的方法。

在自然语言处理领域，BERT模型得到了广泛的应用。为了进一步理解这些模型的功能，(van Aken等人，2019)给出了transformers表示的分层分析。(Devlin et al.， 2018)演示了如何利用预先训练的模型来解释自然语言。基于bert的文本情感识别模型的概述可以在(Acheampong等人，2021年)中找到。在(Xin et al.， 2020)中显示了一种提前离开BERT的修正，以更快地进行推理。

我们早期在情境情绪和情绪分析方面的研究工作使用了集成技术和Gaussian过程模型(Angel Deborah等人，2019年)，(Angel Deborah等人，2021年)，(Rajalakshmi等人，2018年)，(Rajendram等人，2017b)， (Rajendram等人，2022年)和(Rajendram等人，2017a)，形成了抑郁检测的基础。我们使用transformers模型及其变体来检测文本中的冒犯和幽默(Sivanaiah et al.， 2020)、(Sivanaiah et al.， 2021)和(Nanda et al.， 2021)。

### 数据集

任务数据集包含英语社交媒体文本。该数据集包含3列，pid，英文社交媒体文本，以及“未抑郁”、“中度抑郁”和“严重抑郁”的标签。测试、开发和训练数据集都有与这3类相关的数据。该训练集共有8891个条目，其中1971个被标记为“不抑郁”，6019个被标记为“中度抑郁”，剩下的901个被标记为“严重抑郁”。发展组共有4496个条目，分为1830个“不抑郁”，2306个“中度抑郁”和360个“严重抑郁”。测试集有3245个数据点。

### 系统框架

系统第一步是数据预处理，目的是删除文本中不必要元素，并将数据转换为统一形式。具体如下：

扩展缩略词——缩略词是单词的缩写形式，比如don 't，代表do not, aren 't，代表are not。为了使模型更好地执行，我们需要在文本数据中扩展这种收缩。

大小写统一

删除标点符号，使用正则表达式和字符串模块将标点符号替换为空字符串。

删除包含数字的单词和数字——有时文字和数字是写在一起的，机器很难理解。因此，我们必须排除那些由单词和数字混合而成的术语，例如game57或game5ts7。因为这类术语很难处理，所以最好删除它或用NULL字符串替换它。

删除停止词——停止词是文本中最经常出现的、没有提供有用信息的词。停止词包括像他们，他们，谁，这，和那里的词。

词干词干化和词干化——词干词干化是将一个单词还原为其词干的过程，例如run, running, runs和runed，它们都来自于同一个单词。例如，像ing、s和es这样的词，词根是用来去掉前缀和后缀的。这些单词是使用NLTK包生成的。

删除空白

数据增强——这种技术用于创建合成数据，以解决数据集中的不平衡问题。

系统的下一部分是BERT分类模型。该模型使用了来自simpletransformer API的预训练BERT模型。BERT模型对处理过的数据进行了微调，以给出一个3类分类模型，能够将遇到的新数据有效地分类为各种类别。BERT模型分为预训练和微调两个阶段。

数据以pandas数据帧的形式导入。这个数据帧首先被传递给一个函数来展开所有的缩略语，这是通过一个预收集的缩略语字典来完成的。接下来，句子被转换为小写，使用regex编译表达式删除标点符号。此时的数据将被解析为英文stopwords，其列表从python中的自然语言工具包(NLTK)中获得。这些停止字被删除。词干词干和词形化也使用python的NTLK来完成。使用正则表达式删除空白。NLPAUG库用于数据增强，以平衡3个类之间的数据，因为数据在标签之间是不平衡的。

![image-20230509155615025](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230509155615025.png)

## conclusion

综上所述，我们的研究工作提出了一个BERT模型来将社交媒体文本分类为3个目标类。当前的模型在给定的数据上执行得不是很好。其中一个原因可以归结为不同文本的复杂性，不同部分涉及不同的情绪。未来的模型，将通过根据句子的复杂程度来分割句子，并针对不同的复杂程度使用不同的模型来弥补这一缺陷。结果低的另一个原因可能是不同的人表现出不同的抑郁症状，这将通过使用各种其他特征和社交媒体文本来补救。未来可能会提供一个分类器，根据文本的复杂度和情感表达的数量对文本进行分类，从而进一步提高分类器的效率。