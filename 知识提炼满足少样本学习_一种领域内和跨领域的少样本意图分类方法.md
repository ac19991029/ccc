# Knowledge Distillation Meets Few-Shot Learning: An Approach for Few-Shot Intent Classification Within and Across Domains

# 知识提炼满足少样本学习:一种领域内和跨领域的少样本意图分类方法

期刊：Association for Computational Linguistics

## abstract

基于transfomers的大型自然语言理解模型已经在对话系统中取得了最先进的性能。然而，用于训练的标注数据稀缺，模型规模大，推理速度慢，阻碍了它们在低资源场景中的部署。引入少样本学习和知识提炼技术，分别减少了对标记数据和计算资源的需求。然而，这些技术是不兼容的，因为少样本学习训练模型使用很少的数据，然而，知识提炼需要足够的数据来训练较小的、在有限的计算资源上运行的竞争模型。在本文中，我们解决了在目标分类任务的几个样本设置下提取可推广的小模型的问题。考虑到域内和跨域的少量学习场景，我们引入了一种方法，通过使用少量标记的例子，提炼出小模型，将其推广到新的意图类和领域。我们对公共意图分类基准进行了实验，并观察到小模型和大模型之间的性能差异。总的来说，我们在两种少样本场景下的结果证实了小提炼模型的泛化能力，同时具有更低的计算成本。

## introduce

基于transformer的语言模型，如BERT (Devlin et al.， 2019)，对对话系统的开发做出了广泛的贡献。这些系统开发中的一个关键组件是自然语言理解(NLU)，例如意图分类(IC)。意图分类是指在对话系统中确定说话人在特定领域的话语意图。最近，基于bert的语言模型通过对特定任务的数据集进行微调，在意图分类方面实现了最先进的性能(Chen等人，2019)。然而，在面向任务的对话系统中，基于bert的意图分类模型的开发面临两个主要挑战。首先，跨多个领域训练这样的模型需要从多个领域标记训练数据。由于缺乏大量的多领域训练数据，少样本学习(FSL)方法，如基于度量的元学习技术(Vinyals等，2016;Snell等人，2017)，已经被用于将基于bert的意图分类模型适应于新的领域(Li等人，2021)。在跨领域的少样本学习方法中，该模型从大规模的源领域数据中学习可转移的知识，并利用少量的训练样本将知识推广到不可见的目标领域。

第二个挑战是基于transformer的模型的大模型尺寸和长推理时间，这阻碍了此类模型在有限的计算资源下的部署。缩小模型规模的方法，如知识提炼(KD;Hinton等人，2015)，已经被引入。研究表明，新的压缩模型保持了较高的性能百分比，同时比原始模型具有更短的推理时间(Liu et al.， 2019)。任务特定知识提炼方法需要足够大的训练数据集(Tang et al.， 2019)，最好带有标签(Hinton et al.， 2015)，以精馏出一个强大的小模型。然而，为了同时获得广义模型和小模型，知识提炼方法似乎与小概率学习不相容，因为需要大量的训练数据。因此，将知识提炼方法应用于小概率学习是十分必要的。据我们所知，在跨域少样本学习中，任务特定知识的提炼在很大程度上仍未被探索，计算机视觉中也有一些例外(Zhang et al.， 2020b;Li et al.， 2020)和自然语言处理(NLP;Pan等人，2021年;Zhou等人，2021)。

在本文中，我们提出了一种任务特定的方法，在两个少量的学习场景中，提取具有泛化能力的小模型到新的类和领域:1)**单领域和多领域意图分类中的域内目标类泛化**;2)**多域意图分类中的目标域适应**。

为此，我们首先使用元学习在源类和域上预训练一个基于transformer的原型教师网络(Snell等人，2017)。然后，我们设计了一个原型学生网络，利用知识提炼将可转移知识传递给学生。

在提炼过程中，我们将原型损失作为标准提炼损失函数中的一个新的分量。这种损失衡量的是学生模型产生的每个原型与教师模型产生的每个原型的相似程度。此外，相对于知识提炼中的标准批量培训，我们引入了一种情景提炼过程。通过这种方式，我们获得了一个小型的学生模型，该模型兼容于较少样本的场景，并将其推广到看不见的目标类和领域。

#### 创新点

1)通过引入情景提炼过程，利用原型提炼损失，提出了一种新的知识提炼方法。我们的新方法结合了少单元学习和知识提炼的优点。2)我们在四个公共NLU基准上进行了大量的实验，并在少样本的意图分类场景中比较了提炼的小模型和大模型。结果表明，小模型的性能略有下降，同时具有更低的内存消耗和更快的推理速度。3)实验结果表明，小模型能有效地泛化和适应目标领域，且无需教师监督。对于小型学生模型来说，这是一个更具挑战性和现实的场景。

## related work

#### 少样本学习

少样本学习在自然语言处理中得到了广泛的关注。FSL中一个突出的技术是元学习，例如基于度量的元学习技术(Vinyals等人，2016;Snell等人，2017)。在这些技术中，模型是在有足够标记实例的源训练任务上进行训练的，称为元训练;模型是在只有少量标记实例的基础上泛化或适应新任务的，称为元测试。元训练步骤是通过章节来完成的。在每个章节中，每个任务都会选择一组N类(N种方式)。对于每个类，一个包含K个标记示例的支持集和一个查询集被创建用于训练和评估用于更新模型参数的分类器的性能。学习过程以N-way K-shot分类任务的形式进行。在元测试过程中，使用一些有标记的例子来适应新任务，这与元训练类似。

#### Knowledge distillation（知识提炼）

知识提炼方法将一个训练有素的大模型(称为教师)的知识和泛化能力转移到一个小模型(称为学生)(Ba和Caruana, 2014;Hinton等人，2015)。在最简单的情况下，提炼的目标函数是最小化教师和学生预测的软标签之间的差异。作为一种替代方法，logit，也就是最终的softmax函数的输入，可以用来代替软标签来训练学生(Bucila等人，2006)。Hinton等人(2015)教师和学生模型可以有不同的架构。例如，Liu等人(2019)探索了基于变革者的教师和基于变革者和基于lstm的学生模型在NLP中进行多任务知识提炼。KD在基于变压器的教师模式中得到了特别关注，以培训轻量级普通学生(Sanh等人，2019;Sun et al.， 2019;焦等，2020;Sun等人，2020年;Wu等人，2020年)和具有实际应用的特定任务学生(Tsai等人，2019年;Liu et al.， 2019;Clark等人，2019)，包括意图分类(Jiang等人，2021)。

#### 知识提炼和少样本学习

在NLP模型中，知识提炼对于提高新类和新域的整体效率和泛化能力在少样本学习场景下不是很直观。最近的调查表明，由于模型容量更高，较大的模型比较小的模型表现出更好的少样本性能(Brown et al.， 2020)同时，知识提炼需要足够大的训练数据，理想情况下需要有标签(Hinton et al.， 2015)，以精馏出一个具有较小性能差距的小模型。因此，将少样本学习和知识提炼相结合的方法似乎是相互矛盾的。

在计算机视觉的fewshot学习场景的上下文中应用知识提炼的尝试并不多见(Zhang et al.， 2020b;Li et al.， 2020;刘等，2020)。据我们所知，NLP的尝试仅限于Pan等人(2021)和Zhou等人(2021)的工作。在他们的工作中，Pan等人(2021年)训练了一个基于多领域转换的元教师，并引入了一种元精馏方法来获得特定领域的学生模型。与我们的工作类似，他们在提炼过程中考虑了域内泛化和目标域适应场景。但是，我们关注的是一个更具挑战性的场景，在这个场景中，学生模型不能访问新兴领域中的教师。也就是说，学生在不经过任何提炼过程的情况下，用少量带标签的例子来适应新的目标域。因此，我们的模型架构不同于Pan等人(2021)，以保持模型的泛化和适应能力。Zhou等人(2021)提出了一种知识提炼的元学习方法，在这种方法中，教师和学生都是通过相互交流来训练的。教师通过接收关于学生在一种叫做测验集的新数据分割上的表现的反馈来学习提高其迁移能力。在低资源环境下，KD的替代方法考虑数据增强，以生成未标记的数据，并使用增强数据提取小模型(Melas-Kyriazi等人，2019)。

## approach

我们首先描述教师和学生模型架构，然后是我们提出的模型训练过程。我们详细阐述了提出的情景提炼过程的细节，并展示了我们的方法如何在少样本学习场景下保持提炼模型的泛化能力。

### 模型架构

我们考虑的是小样本的学习场景，所以教师和学生模型都被设计成一个圆形网络（Protonet; Snell et al. 2017），这是一种基于度量的元学习方法。

具有可训练参数θT的教师质子网T由一个编码块组成，该编码块是一个具有L层(L >= 2)的Transformer，后面是两个线性隐藏层。该网络的目标是通过训练模型参数θT来学习度量空间。老师的输入是一个序列x = t1…Tk与k个令牌。固定长度的编码序列是来自Transformer最后一层输出的令牌嵌入的平均池化公式如下。然后e(x)作为隐含层的输入，输出为m维序列表示。给定C类，T计算m维类表示rc∈Rm对于C∈{1，…， C}，称为prototype，作为各自类中支持实例的多维表示的平均聚合。对于每个新序列，分类是通过计算类原型和创建的m维序列表示之间的欧氏距离来完成的。

![image-20230511160018885](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230511160018885.png)

具有可训练参数的学生质子网络S θS由两个编码层的Transformer组成，后面是两个线性隐藏层。Transformer层从教师编码块的前两层初始化。类原型的计算方法与教师相同。在这两个体系结构中，所有的模型参数都是可训练的，并且在多领域意图分类的所有领域中共享。

### 模型训练与测试

受元学习的启发，我们实现了元训练和元测试步骤。在我们的工作中有两个场景，我们相应地调整这些步骤。第一个场景是域内类泛化，第二个场景是多域分类中的目标域自适应。由于FSL和KD方法的共同作用，元培训包括两个步骤:1)教师对源课程(领域)的前培训，称为情景前培训;2)学生对源课程(领域)的前培训，使用提出的情景知识提炼，称为情景提炼。在元测试中，我们为第二个场景实施了一个额外的目标领域适应步骤，称为迷你情景适应。

#### 情景构建

假设分别有用于元训练和元测试的源类Ctrain和目标类Ctest的不相交集。这些集合属于源域和目标域拆分、Dtrain和Dtest。在域内目标类泛化场景中，Dtrain = Dtest。为了构造一个情节，域d被一致地从域Dsplit中选择，其中分裂是训练或测试。然后，根据Krone等人(2020)和Triantafillou等人(2020)的工作，我们通过从选定的域d中采样方法n、支持样本ks和查询样本kq的数量来创建可变大小的剧集。然后从域拆分中采样每个类c的支持集Sc和查询集Qc。正如Krone等人(2020)的工作中所讨论的，通过设置每集可变的样本和方式，我们的方法更符合真实世界的情况，在数据集中可以使用不平衡类。章节构建详见附录A.1。元训练由epoch组成，每个epoch包含不同的事件。因此，根据Krone等人(2020)的研究，一旦一个章节被构建，我们就会从元训练分割中去除相应的样本，直到所有的样本都出现在一个epoch中。

#### 情景预训练

为了对教师进行源类(域)的预培训，我们实现了标准的元学习方法。在每个步骤中，通过描述的可变片段构建方法创建一个片段。然后，利用每个类Sc的标记支持集计算类原型rc∈Rm:

![image-20230511162217316](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230511162217316.png)

接下来，该模型计算**每个查询示例表示与类原型之间**的平方欧几里德距离的负值，表示为logit。最后，我们使用查询集的计算logits与查询标签y之间的交叉熵损失作为分类损失:

![image-20230511162348696](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230511162348696.png)

使用Adam优化器更新模型参数θT

#### 情景提炼过程

我们的目标是获得高效的小型学生模型，将其推广到不可见的类(域)。我们结合了FSL和KD的优势，引入**情景知识提炼**作为我们方法的主要组成部分。它在**学生对源类(域)的预训练**步骤中执行。

给定一个在源类(域)上受过训练的教师T，我们提取同一个类(域)上的学生S。提炼过程分为几个阶段。在一个迭代的每一个提炼步骤中，我们都创造了一个情景。支持集用于计算T和S中的类原型，用于对查询集进行分类。我们定义总的提炼损失函数如下

![image-20230511164817675](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230511164817675.png)

其中Lsoft为查询集上教师输出层和学生输出层软标签之间的Kullback-Leibler (KL)散度，计算方法如下:

![image-20230511164954392](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230511164954392.png)

为了转移教师的泛化能力，我们在提炼损失函数中使用了一个新的术语Lpt，它是专门针对少样本学习设置的。Lpt计算T和s中班级原型之间的差异。它计算为教师和学生在班级原型上的平均平方误差(MSE)

![image-20230511165109084](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230511165109084.png)

计算损失后，更新学生模型参数θS。注意，在提炼过程中，学生不能访问查询集标签。

#### 最小情景适应

在多域意图分类中，预先训练的教师模型和学生模型都可以适用于目标域。然而，我们假设模型只能访问少数标记好的例子。为此目的，类似于情景前训练步骤，标准元学习方法被用于使教师或学生适应目标领域。为了模拟少样本的假设，我们从最初由Luo等人(2017)设计的情景中创建迷你情景。在每个适应步骤中，类似于k-fold交叉验证方法，所创建的情景的支持集中的ks实例被重复地分割为n-way ks−1迷你支持实例和一个迷你查询实例。模型参数在每一迷你情景后更新。在推理时，将保留该集的查询集用于求值。我们使用迷你情景适应来调整教师。学生也可以在没有老师监督的情况下使用相同的程序。

#### 元测试

模型性能在元测试时间通过元测试分裂Ctest的随机测试片段进行评估(Krone等人，2020年;Li等人，2021)。在第一个场景中，我们在每个随机测试集中使用支持集和查询集，分别用于原型计算和性能评估。在第二个场景中，我们使用迷你情景适配将模型调整到目标领域，并使用迷你支持集和迷你查询集更新模型参数。然后，集的支持集用于原型计算，而查询集用于性能评估。如果模型在目标域上评估而没有任何适应性，我们将使用支持和查询集来进行原型计算和性能评估

## experiment

我们在公共意图分类数据集上进行了大量的实验来评估所提出的方法。我们模拟了两种场景:域内目标类泛化和更具挑战性的场景——多域目标领域自适应

#### 实验设置

我们在实验中使用了四个公共NLU基准:SNIPS (Coucke et al.， 2018)、ATIS (Hemphill et al.， 1990)、TOP (Gupta et al.， 2018)和clininc150 (Larson et al.， 2019)。为了模拟意图分类中的少样本类泛化，使用了分割。从每个数据集中的类创建一个元训练分割(训练分割)和一个元测试分割(测试分割)。

为了模拟少样本域适应，我们使用Li等人(2021)提出的分割。我们只从ATIS的测试分割中删除了atis_day_name意图，因为它只包含两个语句。对于第二个场景，我们使用Work、Banking和Credit card域作为源域，Home and Kitchen and Dining作为Clinc150数据集中的目标域。我们选择这种分割来最小化源域和目标域之间的重叠。此外，我们没有利用任何验证集来优化模型参数。通过这种方式，我们增加了难度级别，以便在较少样本的场景中进行有意义的比较。此外，SNIPS实际上是一个多领域的数据集，包含跨领域的意图类，而ATIS和TOP是高度不平衡的，因此在较少样本的设置下很难进行比较。TOP还包含了导航和事件域的各种意图类。

在所有场景中，我们在训练前和提炼过程中使用Adam优化器，学习率为1e - 5。在(Krone et al.， 2020)和(Li et al.， 2021)的实验设置之后，教师和学生的培训时间都设定为30个。在测试时间，我们报告了在三个随机种子和100个随机测试片段上的模型的平均精度和标准偏差。我们使用bertbase_un作为基本语言模型，隐藏大小为768。根据实际实验，将质子网络中所有隐藏层和输出特征m设置为200。

在领域内目标类概化中，我们使用情景训练对教师进行预训练，情景的最大支持集大小(Kmax)为20和100。这样，我们就可以直接将我们的结果与Krone等人(2020)的结果进行比较。在目标领域适应情境中，我们报告了经过提炼的学生在目标领域中没有适应和有10个小情景适应阶段的结果。根据Li et al.(2021)的工作，方法n被设置为学生在目标领域适应过程中目标领域中意图类的数量。此外，在适应过程中我们将ks和kq都修改为10。因此，这一步不使用变集结构。培训前和情景提炼步骤与以前一样。

#### 域内目标类泛化

对于每个域数据集，我们通过情景提炼过程，在分割的训练序列上训练一个教师模型，并使用情景提炼过程提取域内的学生。然后我们评估学生在看不见的意图类上的表现，即测试分裂，在各自的数据集没有进一步的调整。

![image-20230517100117711](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230517100117711.png)

从表1中可以看出，领域专业学生的平均成绩达到了领域专业教师的95.7%，在小模型容量有限的情况下证明了其泛化能力。该学生的Kmax = 20和Kmax = 100的平均成绩分别比基线BERT+Proto模型高出5.6分和1.75分。请注意，Krone等人(2020)提出了transformers和插槽填充任务的联合少样本学习方法，这导致了一个更具挑战性的最终任务。因此，为了公平起见，我们避免将老师的成绩与他们的模型进行比较。Kmax越大，学生的性能提升2.4分。由于ATIS中训练类和测试意图类在语义上存在重叠，学生表现出与老师的竞争表现。SNIPS包含语义上遥远的类。同样的，TOP除了高度不平衡之外，还包含了多样化的意图类，这就解释了学生和教师在这些数据集上的表现差距。

表2显示了多领域意图类泛化的结果。

![image-20230517102735670](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230517102735670.png)

表2显示，从多领域教师中提取出的多领域和特定领域的学生，分别达到了教师绩效的82.31%和92.06%。正如预期的那样，多领域学生的表现比领域特定学生平均低7.35分，因为它的表征能力在几个领域是有限的。然而，多领域学生在ATIS领域的表现优于基线BERT+Proto。这表明，当测试集高度不平衡时，如ATIS数据集，多领域训练是有益的。与基线BERT+Proto相比，领域特定学生在6个实验中有4个实验取得了更高的成绩，而在其他2个实验中平均落后1.79分。因此，在通过部署多领域的小模型来减少内存消耗和通过在应用程序中部署几个不同领域特定的模型来获得更高的精度性能之间需要进行权衡。在我们的模型中，可以观察到Kmax = 100时的轻微改进。

#### 目标域适应

在这个实验中，一个多领域的教师被预先训练在源领域(预先训练的T)，一个学生被提炼在源领域使用情景知识提炼(预先训练的S)。为了评估学生在不可见领域的泛化能力，我们使用迷你情景适应将学生适应到一个没有教师访问的目标领域(改编的S)。我们使用小情景改编将其与适应各自领域(适应T)的教师的表现进行比较。

![image-20230517103725660](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230517103725660.png)

表3显示了三个目标域上的平均结果。我们还报告了Li等人(2021)提出的两个跨域模型的结果，称为Base质子网和Base best。Base质子网采用BERT作为编码块，其大小与我们的教师模型大致相同。Base best是不同模型之间得到的最优结果。可以看出，在没有老师监督的情况下，适应的学生比训练前的学生有显著的进步。它也达到了改编教师成绩的95%，甚至在SNIPS上略高于改编教师。此外，适应的学生的表现比大基线学生平均高出7.03分。这导致我们的结论，我们所提出的方法带来的好处，在少样本泛化问题上的小提炼模型和有限的表征能力。

![image-20230517104101134](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230517104101134.png)

我们使用Clinc150数据集扩展了实验，它是一个平衡的数据集。表4给出了Clinc150目标域分割的评估结果。在同样的讨论之后，受过培训的教师比受过培训的学生表现更好。适应学生的正确率高于事先训练的学生

![image-20230517105930261](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230517105930261.png)

在Clinc150源域上进行了预先训练，并在Clinc150- home目标域上进行了测试，我们分别在支持镜头ks = 10和ks = 70的FSL和多镜头学习(MSL)场景下对这些模型进行了测试。在这两种情况下，从老师到学生的准确率都有所下降(15.19分和16.60分)，但差别很小。因此，在少镜头或多镜头的学习设置中，经过蒸馏的学生损失了与教师几乎相同数量的表现准确性。这表明了在FSL背景下提出的情景提炼过程在知识转移中的有效性。此外，教师和学生模型的MSL和FSL的性能损失差异较小(9.63−8.22 = 1.41分)。这意味着提出的方法能够获得可推广的小模型。

## conclusion

讨论了元学习和知识提炼的非平凡合并问题。我们所提出的方法是将大型的基于transformers的模型提炼成较小的学生模型，并在意图分类中兼容较少样本的学习场景。通过情景知识提炼的多步骤元训练，我们获得了一个小的提炼模型，它是可推广的，并且只使用几个带标签的例子就能适应新的类和领域。我们的目标领域适应结果表明，小模型可以在没有教师监督的情况下有效地适应新的领域。当时间和计算资源有限时，这消除了对大型教师的需求。与大模型相比，我们观察到提炼后的模型有轻微的性能损失和更少的内存消耗。综上所述，我们的研究结果让我们了解了联合少样本学习和知识提炼方法的优点和局限性，从而促进了该领域未来的研究。