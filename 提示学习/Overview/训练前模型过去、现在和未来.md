# 训练前模型:过去、现在和未来

## Abstract

近年来，BERT和GPT等大型预训练模型取得了巨大的成功，成为人工智能(AI)领域的一个里程碑。由于复杂的训练前目标和庞大的模型参数，大规模PTMs能够有效地从大量已标记和未标记的数据中捕获知识。通过**将知识存储到巨大的参数中，并对特定的任务进行微调，在巨大的参数中隐含的丰富知识可以使下游的各种任务受益**，这一点已经被实验验证和实证分析广泛证明。现在AI社区的共识是采用PTMs作为下游任务的骨干，而不是从零开始学习模型。在本文中，我们深入研究了**前训练的历史**，特别是它与迁移学习和自我监督学习的特殊关系，以揭示ptms在人工智能发展谱系中的关键地位。此外，我们全面回顾了PTMs的最新突破。这些突破是由计算能力的激增和数据可用性的增加所驱动的，朝着四个重要方向发展:**设计有效的架构，利用丰富的上下文，提高计算效率，以及进行解释和理论分析**。最后，我们讨论了PTMs的一系列有待解决的问题和研究方向，希望我们的观点能够对PTMs的未来研究有所启发和推动。

## Introduction

深度神经网络如卷积神经网络（CNN）、循环神经网络（RNNs）、图神经网络（GNN）和注意力神经网络。近年来已广泛应用于各种人工智能(AI)任务。与以往的非神经模型主要依赖于手工制作的特征和统计方法不同，神经模型**可以从数据中自动学习低维连续向量(即分布式表示)作为特定任务的特征**，从而摆脱复杂的特征工程。尽管深度神经网络取得了成功，但许多研究发现，它们面临的关键挑战之一是**数据匮乏**。由于深度神经网络参数较多，容易过拟合，泛化能力差(Belkin et al.， 2019;Xu等人，2021)缺乏足够的训练数据。

考虑到这个问题，在开发深度神经网络的同一时期，大量的努力被投入到人工智能任务手工构建高质量的数据集，使得学习特定任务的有效神经模型成为可能，这些神经模型优于传统的非神经模型。然而，**手工注释大规模数据是昂贵和耗时的**。更普遍地说，特定AI任务的数据集通常有一个有限的大小。因此，长期以来，如何在有限的人工标注数据下训练出有效的深度神经模型，一直是一个关键的研究课题。

这个问题的一个里程碑是**迁移学习**的引入(Thrun和Pratt, 1998;潘、杨，2009)。人类**可以学习用很少的样本来解决新问题，而不是用大量的数据从头开始训练模型**。这一惊人的学习过程是由人类可以使用之前学到的知识来处理新问题这一事实所推动的。受此启发，迁移学习形成了一个两阶段的学习框架:**从一个或多个源任务中获取知识的训练前阶段，以及将获取的知识转移到目标任务中的微调阶段**。由于在训练前阶段获得了丰富的知识，因此微调阶段可以使模型在样本有限的情况下很好地处理目标任务。

迁移学习为缓解数据饥饿的挑战提供了一种可行的方法，并很快被广泛应用于计算机视觉领域。

自然语言处理(NLP)社区也意识到了PTMs的潜力，并开始为NLP任务开发PTMs (Qiu et al.， 2020)。为了充分利用大规模的无标记语料库为NLP任务提供通用的语言知识，NLP社区采用了**自我监督学习**(Liu et al.， 2020b)来开发PTMs。自我监督学习的动机是**利用文本中的内在关联作为监督信号**，而不是人的监督。

通过自我监督学习，可以利用大量的无标记文本数据来获取通用的语言知识，而不需要劳动密集型的工作量。

长期以来，**消失或爆炸梯度**的问题(Bengio et al.， 1994)是使用深度神经网络进行NLP任务的痛点。因此，在CV社区推进深度PTMs的研究时，NLP社区的早期探索主要集中在对浅层网络进行预处理以捕获单词的语义意义.尽管这些预先训练好的词嵌入在各种NLP任务中发挥着重要作用，但由于每个词仅由一个密集向量表示，因此**在不同语境下对多义词的表示仍存在很大的局限性**。

随着深度神经网络在NLP领域的发展，transformer (Vaswani et al.， 2017)的引入，使得为NLP任务训练非常深度的神经模型成为可能。以**transformer为架构，以语言模型学习为目标**，深度PTMs GPT (Radford and Narasimhan, 2018)和BERT (Devlin et al.， 2019)在2018年被提出用于NLP任务。

从GPT和BERT中我们可以发现，当ptm的大小变大时，具有数亿个参数的大规模ptm可以从文本中捕获多义词消歧、词汇和句法结构以及事实知识。通过对大量样本的pms进行微调，pms丰富的语言知识使其在下游NLP任务上表现出色。如图1(a)和图1(b)所示，在过去的几年中，大规模的PTMs在语言理解和语言生成任务上都表现良好，甚至达到了比人类性能更好的结果。如图2(a)所示，所有这些在NLP社区的努力和成就，让大规模的PTMs成为人工智能研究的焦点，在上一波PTMs允许CV社区取得巨大进展之后。

![image-20230911152730754](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230911152730754.png)

![image-20230911152849068](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230911152849068.png)

针对特定的人工智能任务对大规模PTMs进行微调，而不是从零开始学习模型，这也已成为共识,而具有数千亿参数的GPT-3 (Brown et al.， 2020)的出现，让我们看到了分布在大量模型参数中的潜在力量，尤其是人类的少样本学习能力(如图3所示)。

![image-20230911153054654](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230911153054654.png)

然而，关于PTMs的几个基本问题仍然存在:我们仍然不清楚隐藏在大量模型参数中的本质，训练这些庞然大物的巨大计算成本也阻碍了我们进一步的探索。

我们试图追溯PTMs的发展历史，并绘制其在人工智能谱中的位置，这可以让我们清楚地了解PTMs的核心研究问题。然后，我们介绍了各种最新的PTMs的细节，以下是目前正在推进的四个重要路线，**包括设计有效的架构，利用丰富的上下文，提高计算效率，并进行解释和理论分析**

## Background

### 迁移学习和监督前训练

迁移学习的研究很大程度上是由于人们可以依靠以前学到的知识来解决新问题，甚至取得更好的结果。更正式地说，迁移学习的目的是从多个源任务中获取重要知识，然后应用到目标任务中。

在迁移学习中，源任务和目标任务可能具有完全不同的数据域和任务设置，但处理这些任务所需的知识是一致的。因此，选择一种可行的方法将**知识从源任务转移到目标任务**是非常重要的。为此，提出了各种预训练方法，作为源任务和目标任务之间的桥梁。具体来说，这些方法**首先对多源任务的数据进行预训练，对知识进行预编码，然后再将预编码的知识传递到目标任务的训练模型上。**

迁移学习的两种预训练方法是**特征迁移和参数迁移**。特征迁移方法**预先训练有效的特征表示，以预编码跨领域和任务的知识**,通过将这些预先训练的表征注入到目标任务中，可以显著提高目标任务的模型性能。

参数迁移方法遵循一个直观的假设，即**源任务和目标任务可以共享模型参数或超参数的先验分布**。因此，这些方法**将知识预编码为共享的模型参数**,**然后利用目标任务的数据，通过微调预训练参数来传递知识**

**表示传递和参数传递**都是PTMs的基础。词嵌入是一种建立在特征迁移框架上的语言处理方法，被广泛用于自然语言处理任务的输入。

### 自我监督学习和自我监督前训练

迁移学习可以分为四个子设置:归纳迁移学习 转导迁移学习 自学和无监督迁移学习

在这四种背景中，**归纳和转换**背景是研究的核心，因为这两种背景的目的是**将知识从有监督的源任务转移到目标任务**。虽然监督学习一直是机器学习研究的核心问题之一，但无标记数据的规模要比人工标记数据大得多。近年来，越来越多的研究人员注意到大规模无标签数据的重要性，并致力于从无标签数据中提取信息。自监督学习已经被提出**利用输入数据本身作为监督，从大规模的无标记数据中提取知识**。

自我监督学习和非监督学习在环境上有许多相似之处。在一定程度上，自我监督学习可以被视为无监督学习的一个分支，因为它们都使用了无标记数据。然而，无监督学习主要**侧重于检测数据模式**(如聚类、社区发现和异常检测)，而自监督学习仍处于**有监督设置范式**

NLP任务早期的PTMs以众所周知的词嵌入形式存在,该方法应用自监督方法将单词转换为分布式表示。由于这些预先训练的词表示**捕获了文本中的句法和语义信息**，它们经常被用作NLP模型的输入嵌入和初始化参数，并比随机初始化参数有显著改进,由于这些词级模型经常出现词多义现象，Peters等人(2018)进一步**采用序列级神经模型来捕获跨不同语言语境的复杂词特征，并生成上下文感知的词嵌入**。使用单词嵌入作为神经模型的输入几乎已成为NLP任务的常见模式。

提出处理顺序数据的transformer后，NLP任务的PTMs进入了一个新的阶段，因为与传统的cnn和rnn相比，可以训练出更深层的语言模型。与那些用作输入特征的字级PTMs不同，**基于transformer的PTMs(如GPT和BERT)可以用作各种特定任务的模型主干。**在大规模文本语料库上对这些基于Transformerbased的ptm进行预训练后，**ptm的结构和参数都可以作为特定NLP任务的起点**，即针对特定的NLP任务对ptm的参数进行微调，即可获得竞争性能。

在上一波监督前训练之后，自我监督前训练成为当前人工智能研究的焦点。

## Transformer and Representative PTMs

### Transformer

在Transformer之前，rnn一直是处理顺序数据的典型工具，特别是处理自然语言的工具。由于**rnn具有顺序性**，它们**在每个时间步中按顺序读取一个单词**。对于每个单词，**rnn引用其前一个单词的所有隐藏状态来处理它**。这种机制被认为很难利用高性能计算设备(如gpu和tpu)的并行能力。

如图5所示，Transformer是一个由编码器和解码器组成的非循环序列到序列(seq2seq)体系结构。Transformer的编码器和解码器都由几个相同的块堆叠。每个编码器块由多头自注意层和位置前馈层组成。与编码器块相比，每个解码器块都有一个额外的交叉注意层，因为解码器需要将编码器的输出作为生成的上下文考虑。在神经层之间，使用了残余连接使得训练深度transformer成为可能。

![image-20230911162351201](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230911162351201.png)

#### Attention Layer.

自我关注层是Transformer成功的关键。形式上，给定一个查询集Q = {q1，…， qn}，一个键集K = {k1，…， km}，一个值集V = {v1，…， vm}，每一个查询向量qi∈Rdk，每一个键向量ki∈Rdk，每一个值向量vi∈Rdv，所缩放的点积注意定义为

![image-20230911163035475](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230911163035475.png)

Q是计算注意力的向量集合，K是计算注意力的向量集合。作为点积乘法的结果，我们可以得到权重aij来表示查询向量qi与键向量kj之间的关系。最后，我们可以计算值向量的加权平均值作为注意层的最终结果。注意，屏蔽函数ATT-Mask(·)用于限制每个查询向量可以参加哪些键值对。如果不让qi参加kj, ATT-Mask(x) =−∞，否则ATT-Mask(x) = x。

将Q、K、V分别装入矩阵表示Q∈Rn×dk,K∈Rm×dk,V∈Rm×dv，可以简化为

![image-20230911163310971](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230911163310971.png)

其中Softmax(·)以行方式应用，A∈Rn×m为注意矩阵，H∈Rn×dv为结果。

Transformer没有使用普通的、按比例缩放的点积注意力，而是应用了一个定义如下的多头注意力层

![image-20230911163404882](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230911163404882.png)

h是头数。分别用WQ i、WK i、WV i将输入的Q、K、V投影到第i个头注意的特征空间中。通过Concat(·)连接所有头输出后，多头注意层应用WO将连接投影到最终的输出空间中。

#### Position-Wise Feed-Forward Layer

除了注意层之外，Transformer的每个块还包含一个位置前馈层。给定打包的输入矩阵X∈Rn×di表示一组输入向量，di为向量维数，则定义位置前馈层为

![image-20230911163602101](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230911163602101.png)

其中σ(·)为激活函数(通常为ReLU函数)。W1∈Rdi×df, b1∈Rdf, W2∈Rdf×do, b2∈Rdo都是投影的可学参数。H∈Rn×do是前馈层的最终结果。根据经验，di等于do, df远大于di和do。

#### Residual Connection and Normalization

ransformer在各个神经层之间应用了剩余连接和层规范化，使得Transformer的体系结构可能是深层的。形式上，给定神经层f(·)，残差连接和归一化层定义为

![image-20230911163806875](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230911163806875.png)

其中LayerNorm(·)表示层规范化操作。

如图5所示，在Transformer中，多头注意有三种变体:

1. 在编码阶段，给定一个单词，自我注意通过与输入序列中的所有单词进行比较，计算出它的注意分数。这样的注意分数表明了其他每个单词对下一个给定单词的表征应该有多大的贡献。
2. 解码器采用掩面自注意，其注意矩阵满足Aij = 0, i>j，这种注意有利于自回归语言建模。在解码阶段，自我注意与编码相似，只是它一次只从左到右解码一种表示。由于解码阶段的每一步都只参考之前解码的结果，因此我们需要在自注意中加入掩蔽函数。
3. 在解码器中也使用交叉注意，它将前一个解码器块的输出作为Q，将编码器的输出作为K和v。这个过程本质上是对整个输入序列的信息进行聚合，在解码阶段将其应用到所有生成的单词上。利用输入上下文对机器翻译、文本摘要等seq2seq任务具有重要意义。



### GPT

PTMs通常包括两个阶段，培训前阶段和微调阶段。由Transformer解码器作为骨干装备，GPT应用生成式预训练和有区别的微调。从理论上讲，与以往的PTMs相比，GPT是第一个将现代Transformer体系结构和自我监督的培训前目标结合起来的模型。

对于没有标签的大型语料库，GPT优化了一个标准的自回归语言模型，也就是说，**通过将所有单词的前一个单词作为上下文，最大化所有单词的条件概率**。在GPT的前训练阶段，Transformer对每个单词的条件概率进行建模。

对于每个单词，GPT通过对其前面的单词应用掩蔽的多头自我注意操作来计算其概率分布。形式上，给定一个由标记X = {x0, x1，…， xn, xn + 1}， GPT通过最大化以下日志似然来应用标准语言建模目标:

![image-20230912100309699](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230912100309699.png)

![image-20230912100208620](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230912100208620.png)

其中k为窗口大小，概率P由Transformer解码器建模，参数为Θ， x0为特殊令牌[CLS]， xn+1为特殊令牌[SEP]。

GPT对特定任务的适应过程是**使用GPT预先训练的参数作为下游任务的起点进行微调**。在微调阶段，通过GPT传递输入序列，我们可以得到GPT Transformer的最后一层的表示。通过使用最后一层的表示和特定于任务的标签，GPT使用简单的额外输出层优化下游任务的标准目标。

### BERT

BERT采用了双向深变结构作为主体结构。还有两个独立的阶段可以使BERT适应特定的任务，即训练前和微调(

![image-20230912101423928](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230912101423928.png)

在训练前阶段，BERT使用的是自编码语言建模，而不是GPT使用的自回归语言建模。更具体地说，受完形填空(Taylor, 1953)的启发，设计了目标蒙面语言建模(MLM)。如图7所示，在传销过程中，令牌随机被一种特殊的令牌[MASK]掩蔽，**目标是在有上下文的情况下预测掩蔽位置上的单词**。与标准的单向自回归语言建模相比，传销可以实现所有符号的深度双向表示。形式上，给定一个由标记X = {x0, x1，…， xn, xn + 1}， BERT随机掩码X中的m个令牌，使下列对数似然最大化:

![image-20230912101548199](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230912101548199.png)

其中，概率P由Transformer编码器建模，参数为Θ， X ~，是在X中屏蔽某些令牌后的结果，[Mask]i为第i个掩码位置，yi为该位置的原始令牌。

除了MLM外，下一个句子预测的目标(NSP)也被用来捕捉句子之间的语篇关系，用于一些下游的多句任务，如自然语言推理和问答。在这个任务中，使用一个二分类器来预测两个句子是否连贯。在训练前阶段，MLM和NSP共同优化BERT的参数。

通过预训练，BERT可以获得下游任务的鲁棒参数。通过使用下游任务的数据修改输入和输出，BERT可以对任何NLP任务进行微调。如图8所示，BERT可以有效地处理那些输入一个句子或句子对的应用程序。对于输入，**它的模式是两个用特殊标记[SEP]连接起来的句子**，可以表示:(1)释义中的句子对，(2)蕴含中的假设-前提对，(3)问答中的问题-段落对，(4)文本分类或序列标注中的单句。对于输出，**BERT将为每个令牌生成一个令牌级别的表示**，它可以**用于处理序列标记或问题回答**，而特殊的令牌[CLS]可以被输入到额外的层中进行分类。

继GPT和BERT之后，有人提出了它们的一些改进，如RoBERTa和ALBERT。RoBERTa(Liu et al.， 2020d)是BERT的成功变体之一，它主要有四个简单而有效的改变:**(1)去除NSP任务;(2)训练步骤多，批量大，数据多;(3)较长的训练句子;(4)动态改变[MASK]模式。**ALBERT(Lan et al.， 2019)是BERT的另一个重要变体，它提供了一些关于简化参数的有趣观察。首先，它**将输入字嵌入矩阵分解为两个较小的矩阵**。其次，它**强制所有Transformer层之间的参数共享，以显著减少参数**。第三，**提出了句子顺序预测(SOP)任务来替代BERT的NSP任务**。作为空间效率的牺牲，ALBERT具有较慢的微调和推理速度。

## 设计有效的架构

基于transformer的PTMs的成功激发了一系列为自然语言及其他领域建模序列的新架构。一般来说，所有用于语言前训练的bert后Transformer架构都可以根据两个动机进行分类:**统一序列建模和认知启发架构**。

### 统一序列建模

#### 结合自回归和自编码建模。

将gpt风格的单向生成和bert风格的双向理解统一起来的先锋工作是XLNet (Yang等人，2019)，它提出了**置换语言建模**。BERT中的掩码恢复策略自然与它的下游应用相矛盾，后者在输入句中没有[MASK]。XLNet通过**在训练前打乱令牌的顺序，再应用自回归预测范式**解决了这一问题，使XLNet具备了理解和生成的能力。置换语言建模的一个重要追随者是MPNet (Song et al.， 2020)，它弥补了XLNet在训练前不知道句子长度，而在训练后却知道句子长度的差异。

除了置换语言建模，另一个流程是**多任务训练**。UniLM (Dong et al.， 2019)提出将不同的语言建模目标联合训练在一起，包括单向、双向和seq2seq目标。这可以通过改变transformers中的注意力面具来实现。UniLM在生成式问答和抽象总结方面表现得很好。

GLM (Du et al.， 2021)提出了一种更优雅的方法来结合自回归和自编码。**给定一个可变长度的掩码跨度**，GLM要求**Transformer块自回归地生成掩码令牌**，而不是像BERT和SpanBERT (Joshi et al.， 2020)那样提供[MASK]的数量来建模。为了保留[MASK]s的信息，GLM提出了一种二维位置编码策略。

#### 应用广义编码器-解码器

在GLM之前，编码器结构(如BERT)或译码结构(如GPT)都不能解决一个重要问题:**用可变长度填充空白**(Du et al.， 2021;沈等，2020b)。基于解码器的模型不能做到这一点，因为它们**只能在序列的末尾生成**，而基于编码器的模型也不能做到这一点，**因为[MASK]的数量会泄露信息**。一个自然的想法是**转向最初为机器翻译设计的编码器-解码器架构，它将根据源产生可变长度的目标序列。**

MASS (Song et al.， 2019)，它将**隐藏预测策略引入**到编码器-解码器结构中。然而，MASS并没有涉及填充变长空白的问题。T5 (Raffel et al.， 2020)通过**使用一个掩码令牌在文本中屏蔽一个可变长度的跨度**来解决这个问题，并要求解码器恢复整个掩码序列。BART (Lewis et al.， 2020a)引入了一种有趣的思想，即**通过截断、删除、替换、变换和掩蔽(而不仅仅是掩蔽)等多种操作破坏源序列**。

### 认知架构

#### 可维护工作记忆

Transformer的一个自然问题是其**固定的窗口大小和二次空间复杂度**，这严重阻碍了其在长文档的理解和生成方面的应用。

尽管对二次增长点态注意的近似计算进行了一系列修改(Tay et al.， 2020)，但问题是，我们人类并没有出现这样的长期注意机制。作为替代，认知科学家已经揭示人类可以保持工作记忆(Baddeley, 1992;布朗，1958年;Barrouillet等人，2004年;沃顿等人，1994)，它不仅记忆和组织，但也忘记。传统的长短期记忆(LSTM)网络就是这种理念的一个范例。

对于基于transformer的架构，Transformer-XL (Dai等人，2019)是第一个引入**分段级递归和相对位置编码**来实现这一目标的。然而，这种递归只是隐式地模拟了工作记忆。作为一种更明确的解决方案，CogQA (Ding et al.， 2019)提出在多跳阅读中维护认知图。它由两个系统组成:**基于PTMs的系统1和基于gnn的系统2，对认知图进行建模，实现多跳理解**

CogQA的一个限制是，它对系统1的使用仍然基于固定的窗口大小。为了赋予工作记忆理解长文档的能力，CogLTX (Ding et al.， 2020)利用MemRecall语言模型，**选择工作记忆中应该保持的句子和任务特定模块进行回答或分类**

#### 可持续长期记忆

## 利用多源数据

### Multilingual Pre-Training

基于大规模英语语料库的语言模型在许多方面都取得了巨大的成功。然而，我们生活在一个多语言的世界，由于成本和所需的数据量，为每种语言训练一个大型语言模型并不是一个优雅的解决方案。因此，训练一个模型来学习多语言表征而不是单语言表征可能是一种更好的方法。

学习多语言表征的方法主要有两种。一种方法是**通过参数共享来学习**。例如，用几个语言对一起训练多语言lstm，就可以实现多语言翻译。另一种方法是**学习语言无关的约束**，例如使用WGAN (Arjovsky et al.， 2017)框架将语言表示解耦为特定语言和语言无关的表示。

根据任务目标，多语言任务可以分为**理解任务和生成任务**。理解任务侧重于句子级别或单词级别的分类，并有助于下游的分类任务，如自然语言推理(Conneau等，2018b)。生成任务主要关注句子的生成，对于机器翻译等下游生成任务至关重要。

MMLM任务不能很好地利用并行语料库。事实上，平行语料库对于机器翻译等自然语言处理非常重要。从直观上看，平行语料库对于直接学习具有相同意义的不同语言句子的跨语言表征非常有帮助。从这一点出发，XLM (Lample and Conneau, 2019)利用双语句子对来执行翻译语言建模(TLM)任务。与BERT中的传销相似，TLM将两个语义匹配的句子组合成一个句子，并随机屏蔽两部分中的标记。与传销相比，TLM需要模型根据双语环境预测蒙面令牌。这鼓励模型将两种语言的表示组合在一起。

### Multimodal Pre-Training

现有的跨模态预训练PTMs主要集中在:**(1)改进模型架构，(2)利用更多的数据，(3)设计更好的预训练任务。**

### Knowledge-Enhanced Pre-Training

结构化知识的典型形式是知识图。许多工作试图通过整合**实体嵌入和关系嵌入**来增强PTMs.Wang等人(2021b)通过将**语言模型损耗和知识嵌入损耗结合**在一起，基于维基多实体的描述对模型进行预训练，以获得知识增强表示。有些作品将知识图中的路径甚至子图视为一个整体，直接对它们和对齐的文本建模，以保留更多的结构化信息。

由于将实体和关系对齐到原始文本通常很麻烦，并且可能在数据预处理中引入噪声，另一项工作(Bosselut等人，2019年;Guan等人，2020年;Chen et al.， 2020e)可以直接将结构化知识转换为序列化的文本，并让模型自行学习知识-文本对齐。

特定领域或任务的数据可以看作是一种非结构化知识。许多作品(Beltagy等人，2019年;Lee等人，2020)进一步根据这些数据对一般的PTMs进行预训练，以获得更好的特定领域或特定任务的模型。

## 提高计算效率

本节将从系统级优化、高效学习算法、模型压缩策略三个方面介绍如何提高计算效率。

### System-Level Optimization

减少计算需求的一种有效和实际的方法是**系统级的计算效率和内存使用优化**。系统级优化方法通常是模型无关的，并且不会改变底层的学习算法。

#### Single-Device Optimization.

目前大型PTMs的预训练需要大量的内存。这主要是由于浮点数的冗余表示。现代的深度学习系统主要基于单精度浮点格式(FP32)。然而，模型的权值通常在一个有限的范围内，使用半精度浮点格式(FP16)可以完成大部分计算，而精度损失很小

FP16中的训练模型可能会因为浮点截断和溢出而失败。为了解决这一问题，提出了**混合精度训练方法**,在FP32中保留一些临界权值以避免浮点溢出，并使用动态损耗缩放操作来消除浮点截断。

除了浮点数的冗余表示之外，为计算梯度保存的激活状态也是冗余的。例如，在基于transformer的模型中，除了注意层和线性层的权值外，计算设备还存储了每一层的隐藏状态，以提高梯度反向传播中链式法则的效率。与模型参数相比，这些隐藏状态甚至会消耗更多的内存。为了处理冗余的激活状态，使用了**梯度检查点方法**(Rasley et al.， 2020)，通过只存储前向传递后的部分激活状态来节省内存。如果需要，将在向后执行步骤期间重新计算丢弃的激活状态

#### Multi-Device Optimization

目前，分布式训练是一种常用的预训练方法，即利用分布在多个计算节点上的多个gpu对单个模型进行训练。

当对具有数十亿到数万亿参数的模型进行预训练时，传统的数据并行给将整个模型参数拟合到单个GPU带来了挑战，即使是半精度或混合精度的训练。虽然这一问题可以通过使用更大内存的GPU来解决，但费用难以负担，因此普通研究人员对PTM的使用受到了限制。模型并行化是解决这一问题的有效途径(Shazeer et al.， 2018)。如图11所示，在进行模型并行时，可以将模型参数分布到多个节点上。这些节点之间的约简-散点和全集等通信操作保证了前向传和后向传的正确性。

![image-20230912134018616](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230912134018616.png)

### Efficient Pre-Training

#### Efficient Training Methods

传统的训练前任务可能是低效的。例如，对于广泛用于对最近的ptm进行预训练的传销，需要模型**根据上下文预测蒙面令牌**。掩码标记通常是**输入标记的子集(通常是15%)**，也就是说，模型只能从一小部分输入标记中学习。

ELECTRA (Clark et al.， 2020)应用了替换令牌检测任务。这个任务要求**模型区分输入令牌是否被生成器替换**。这个任务可以从每个示例中利用更多的监督信息，因为需要区分所有输入令牌。

#### Efficient Model Architectures

除了有效的预训练方法外，更多的**模型结构变体**也可以降低计算复杂度，从而提高PTMs的训练效率。对于大多数基于transformers的PTMs，随着输入序列的增加，由于序列长度在时间和空间上的二次复杂度，其效率受到注意权值计算的限制。

部分作品(Peng et al.， 2021;Choromanski等人，2021年;Wang et al.， 2020c;Katharopoulos等人，2020)设计**低秩核**，从理论上近似原始的注意权值，导致线性复杂度。有些作品(Child et al.， 2019)通过**将每个令牌的视图限制为固定大小，并将令牌分割成几个块**，从而在每个块中计算注意力权重，而不是一个完整的序列，从而将稀疏性引入到注意机制中。与预定义的块相比，有些作品(Roy等人，2021;Kitaev等人，2020年)发现，使用**可学习参数将令牌分配成块**可以获得更好的性能。另一种方法(Guo et al.， 2019;Lee等人，2019年;Beltagy等人，2020年;Ainslie等人，2020年;Zaheer等人，2020)**结合全局和局部注意机制，然后使用全局节点在序列中收集令牌**。这样，就可以将长序列压缩成少量的元素，从而降低复杂度。

### Model Compression

另一种提高PTMs效率的重要方法是**模型压缩**。在这种设置中，大型模型被压缩成小型模型，以满足在资源受限的设备上进行更快的推理和部署的需求。

#### Parameter Sharing

可以使用跨相似单元的共享参数对PTMs进行压缩。ALBERT (Lan et al.， 2019)使用**分解的嵌入参数化和跨层参数共享来减少PTMs的参数**。在所有Transformer层中使用相同的权值，ALBERT实现了基于BERT模型的显著参数缩减，同时具有相同甚至更好的性能。

#### Model Pruning

为了更好地利用现有ptm的过度参数化特征，另一种降低模型参数的方法是**模型剪枝**。

#### Knowledge Distillation

虽然ALBERT节省了PTMs的内存使用，但它的推理时间并没有显著减少，因为**特征仍然需要以与原始模型相同的数量遍历其层**。知识提炼的目的是**训练一个小模型来再现一个大教师模型**的行为。当使用一个小的经过提炼的模型进行推理时，内存使用和时间开销都减少了。

#### Model Quantization

为了得到更压缩的模型，模型量化也是一种有用的技术.模型量化是指**将精度较高的浮点参数压缩为精度较低的浮点参数**。传统的PTMs通常用**32位或16位**表示，而量化后的模型可以用**8位甚至1位或2位**表示。

## Interpretation and Theoretical Analysis

研究人员还探索解释ptm的行为，包括理解ptm如何工作和揭示ptm捕获的模式。这些工作涵盖了PTMs的几个重要特性:**知识、健壮性和结构稀疏性/模块化**。

### Knowledge of PTMs

与传统的神经网络模型如cnn和rnn具有较少的层数和参数相比，大规模的PTMs可以**从大量的训练前数据中学习丰富的语言知识**。为了研究PTMs的语言知识，研究者设计了几种方法:

1. 表征探测:确定PTMs的参数，针对特定的探测任务，在PTMs的隐藏表示上训练一个新的线性层。
2. 表示分析:利用PTMs的隐藏表示来计算距离或相似度等统计量。根据这些统计数据，我们可以构造出不同单词、短语或句子之间的关系
3. 注意分析:与表征分析类似，注意分析计算注意矩阵的统计量，更适合发现文本的层次结构。
4. 生成分析:使用语言模型直接估计不同序列或单词的概率

#### World Knowledge

除了语言知识外，PTMs还通过前训练学习丰富的世界知识，主要包括**常识知识和事实知识**

### Robustness of PTMs

对抗性攻击的目的是通过**对原始输入的小扰动来生成新的样本**，而这些样本会被模型误分类。

目前的研究主要是利用模型预测、预测概率和模型梯度来搜索对抗实例。然而，要保持机器生成的对抗性实例的质量是困难的。最近，人在环方法(Wallace等人，2019b;Nie et al.， 2020)已经被应用于生成更自然、有效和多样化的对抗性例子，这带来了更大的挑战，暴露了PTMs更多的属性和问题。

### Structural Sparsity of PTMs

在BERT之后，大多数PTMs都采用Transformer作为体系结构主干。尽管使用CNN和RNN可以很容易地训练一个深层次的Transformer，并比以前的工作取得显著的改进，但Transformer遇到了**过度参数化**的问题。

研究表明，在机器翻译(Michel et al.， 2019)、抽象摘要(Baan et al.， 2019)和语言理解(Kovaleva et al.， 2019)的任务中，多头注意结构是冗余的，即**去除部分注意头可以获得更好的性能**。

## Future Directions

### Architectures and Pre-Training Methods

#### New Architectures

transformers的主要局限性在于其计算复杂度。受gpu内存的限制，目前大多数PTMs都不能处理包含512个以上令牌的序列。因此，寻找更有效的模型架构来捕获长期的上下文信息是很重要的。然而，深度体系结构的设计具有挑战性，我们可以寻求一些自动方法的帮助，如**神经体系结构搜索**(NAS)。

#### New Pre-Training Tasks

通用的PTMs一直是我们学习语言内在的普遍知识(甚至是世界知识)的追求。然而，这种PTMs通常需要更深入的架构、更大的语料库和具有挑战性的训练前任务。所有这些要求进一步导致更高的培训费用。此外，大型模型的训练也是一个具有挑战性的问题，这需要复杂而高效的训练技术，如分布式训练、混合精度训练等。因此，根据现有硬件和软件的能力，**设计更有效的自我监督的前训练任务和训练方法**是一个更实际的方向。

#### Beyond Fine-Tuning

微调是将PTMs知识转移到下游任务的主要方法，但其不足之处是**参数效率低下**:每个下游任务都有自己的微调参数。

提示调优是一种很有前途的激发分布在PTMs中的语言知识和世界知识的方法

#### Reliability

### Multilingual and Multimodal Pre-Training

#### More Modalities

除了图像和文本，视频和音频也可以用于多模态的预训练。要处理这个问题，重要的是为更复杂的模式开发更有效和高效的自我监督学习方法。

#### More Insightful Interpretation

最新的深度学习可视化工具可以用于多模态预训练的解释。

#### More Downstream Applications

多模态预训练可以应用于图像-文本检索、图像-文本生成、文本-图像生成等下游任务。然而，要找到一个“真实”的实际应用场景来进行多模态预培训仍然是一个挑战，因为可以使用许多有效的工程技巧(即使成本更低)。

#### Transfer Learning

在培训前添加不可见的语言是不灵活的。因此，应该探索一种新的培训前框架，以轻松适应那些看不见的语言。

### Computational Efficiency

#### Data Movement

开发一个高效的分布式深度学习框架面临着各种挑战。一个人必须小心地管理设备之间的数据移动，否则这可能成为性能瓶颈.需要**定义良好的并行策略，通过最小化通信成本、最大化计算和内存资源以及优化计算-通信重叠，将计算任务放置和调度在互连设备上。**

#### Parallelism Strategies

并行策略的选择，**数据并行、模型并行、流水线并行和各种混合并行方法**可以根据神经网络的结构和硬件配置找到它们的最佳用途

#### Large-Scale Training

#### Wrappers and Plugins

### Modeledge Learning

#### Knowledge-Aware Tasks

虽然符号知识的使用是有效的，但手工组织这些离散的知识，例如构建各种知识库，是耗时和劳动密集型的。

我们可以发现，PTMs捕获了大量的人类知识，并以modeledge的形式存储。如何激发ptm的模型边缘是值得进一步探讨的问题。

#### Modeledge Storage and Management

由于现有的ptm构建在不同的体系结构上，并且可能使用不同的语料库进行训练，因此它们包含不同的模型边缘。因此，如何在PTMs中存储和管理各种连续模型边缘成为一个新的挑战。

有两种简单的想法。第一种是在**超大规模的数据上预先训练一个巨大的模型**。第二种是**将多个模型结合成一个基于专家混合的大模型**(MoE) 

### Cognitive and Knowledgeable Learning

#### Knowledge Augmentation

对于一个输入文本，有丰富的相关外部知识，可以用来扩充输入。考虑到知识和纯文本的格式是非常不同的，重要的是要**弥合文本表示和知识表示(包括符号或向量)之间的差距**，并统一**使用它们的信息作为输入**。解决这一问题需要**统一的模型体系结构和知识引导的培训前目标**。

#### Knowledge Support

当前的模型架构是手工设计的，并且通常非常规则。利用对输入的先验知识，我们可以**训练不同的子模块来处理不同种类的输入**，这样可以加快训练和推理的过程，提高模型的效率。

#### Knowledge Supervision

#### 