# 重要的不仅仅是规模:小的语言模型也是少样本学习

## Abstract

当扩展到数千亿个参数时，像GPT-3 (Brown et al.， 2020)这样的预训练语言模型实现了显著的少样本性能。然而，训练和应用如此庞大的模型需要大量的计算，导致了巨大的碳足迹，使研究人员和从业人员难以使用它们。我们发现，类似于GPT-3的性能可以通过更“绿色”的语言模型获得，因为它们的参数数量要小几个数量级。这是通过**将文本输入转换成包含任务描述的完形填空问题，并结合基于梯度的优化来实现**的;利用未标记的数据提供了进一步的改进。我们确定了使用小语言模型成功理解自然语言所需的关键因素

## Introduction

在大量语料库上训练更大的语言模型(LMs)已经导致了NLP的巨大改进。一种标准的方法是将**预训练模型的输出层替换为特定任务的头部，并根据一组标记的训练数据对整个模型进行微调**。

另一种替代启动的方法是**模式开发训练**(PET)，结合了将任务重组为完形填空的想法和定期的基于梯度的精细调整。虽然PET还需要未标记的数据，但在许多实际应用中，未标记的数据比标记的示例更容易获得。关键的是，PET只有在LM预测的答案对应于词汇表中的单个标记时才会起作用;这是一个严重的限制，因为许多任务不能简单地用这种方式来描述。

## Related Work

很难找到方法将任务重新表述为LMs能够很好理解的完形填空问题(Jiang et al.， 2020)， Schick and Schutze(2021)提出了PET，一种使用知识提炼的方法(Hinton et al.， 2015)和自我训练。我们的PET修改版本使用掩蔽语言模型(Devlin等人，2019)为文本序列分配概率

## Pattern-Exploiting Training

设M为掩码语言模型(MLM)， T为其词汇量，__∈T为掩码令牌;我们将所有标记序列的集合表示为T∗。对于某些z∈T∗包含至少k个蒙版和T∈T，我们用qk M(T |z)表示M在z中的第k个蒙版位置赋给T的概率;在应用softmax之前，模型的对数用sk M(t |z)表示。我们考虑将输入x∈x映射到输出y∈y的任务，为此PET需要一组模式描述器对(PVPs)。每个PVP p = (p, v)包含

1. 一个模式P: X→T∗，将输入映射为包含单个蒙版的完形问题;
2. 一个语言表达器v: Y→T，它将每个输出映射到一个表示模式中特定任务含义的标记。

![image-20231004100010541](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004100010541.png)

如图2所示，PET的核心思想是根据v(y)在P(x)的掩码位置为“正确”标记的概率，得出y为x的正确输出的概率。根据这种直觉，y (x)给定的条件概率分布qp定义为

![image-20231004100252385](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004100252385.png)

其中sp(y | x) = s1 M(v(y) | P(x))为v(y)在P(x)的掩码位置的原始分数。

对于给定的任务，在缺乏大型开发集的情况下，识别性能良好的pvp是一项挑战。因此，PET使能多个pvp的组合P = {p1，…， pn}如下:

1. 对于每个PVP p，一个MLM是通过最小化y和qp(y | x)之间的交叉熵对训练例子(x, y)进行细微调整。
2. 用细调的MLM集合来标注一组未标注的例子;每个未标记的例子x∈x都根据概率分布用软标签进行标注![image-20231004100436008](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004100436008.png)类似于Eq. 1，其中wp是一个权重项，与训练前在训练集上用p实现的精度成正比。
3. 得到的软标记数据集通过最小化其输出和qP之间的交叉熵来训练正则序列分类器。

### PET with Multiple Masks

PET的一个重要限制是，语言表达器v必须将每个输出映射到单个令牌，们将语言表述器推广为函数v: Y→T∗;这需要对推理和训练进行一些修改我们进一步推广PET，因为我们不假设输出空间对于每个输入都是相同的:对于每个x∈x，我们用Yx⊆Y表示给定x作为输入的可能输出集合。给定一个PVP p = (p, v)，我们定义l(x) = maxy∈Yx |v(y)|为表示Yx和Pk(x)为p (x)的任何输出所需的最大令牌数，并将掩码令牌替换为k个掩码。

作为一个运行的例子，我们考虑标签为Y ={+1，−1}的餐厅评论的二元情感分类任务。我们用P(x) = x，it was__。和一个表达器v，将+1映射到单个标记great和−1映射到序列terri•ble，即，我们假设MLM的分词器将单词“terrible”分割成两个标记terri和•ble。在这个例子中，对于所有的x l(x) = 2;P2(x)如图3 (a)所示。

![image-20231004101147975](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004101147975.png)

对于x∈x, y∈Yx和|v(y)| = k，我们用一种自回归的方式重新定义qp(y | x):从Pk(x)开始，我们执行k个连续的预测，在那里我们总是选择下一个令牌来预测基于MLM的信心。即，我们设qp(y | x) = q(v(y) | Pk(x))其中

![image-20231004101246778](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004101246778.png)

with j = arg maxk i=1 qi M(ti | z)， z’是z除了z‘j = tj和t’= t1…Tj−1tj+1…tk注意，与原来的PET(公式1)不同，qp不是一个概率分布，因为它的值和不是1。

对于我们的情感分类示例，图3说明了qp(−1 | x)是如何计算的:当|v(y)| = |{terri，•ble}| = 2时，我们首先使用z = P2(x)来计算v(y)中每个令牌的概率(图3a)。然后，我们选择概率最高的令牌，将其放在对应的掩码令牌的位置，并使用生成的完形填空问题z来计算剩下的令牌的概率(图3b)。y =−1的总分计算为

![image-20231004101853636](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004101853636.png)

对于每个训练例子(x, y)，计算Eq. 3中的qp(y | x)将是非常昂贵的。为了能够在单个转发中计算所有所需的概率，我们通过(i)总是插入表示任何输出所需的最大掩码令牌数来逼近qp(y | x)，并且(ii)对于每个y‘∈Yx，预测v(y’)= t1…Tk中的所有令牌，其中我们简单地忽略模型对所有l(x)−k多余掩模令牌的预测:

![image-20231004102045915](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004102045915.png)

对于我们的运行示例，这意味着我们通过计算近似得分qp(y | x)

![image-20231004102102613](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004102102613.png)

由于~qp不是Yx上的概率分布，交叉熵不是一个理想的训练目标，因为它也可以通过减少分配给序列z /∈v(Yx)的概率来最小化，这些序列不是输出空间的一部分，尽管这对模型的预测没有影响。我们选择了多级铰链损失和最小化

![image-20231004102236968](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004102236968.png)

要求y的log概率与任何输出y‘∈Yx \ {y}的log概率之差至少为1。

## Experiments

我们在SuperGLUE (Wang et al.， 2019)上比较PET和GPT-3，我们通过使用固定的随机种子为每个任务随机选择32个例子来创建新的训练集。还为每个任务创建了多达20,000个未标记示例集;这是通过删除原始训练集的所有标签来实现的。我们将训练示例和未标记示例的结果集称为FewGLUE

### Task

我们将描述每个SuperGLUE任务以及相应的pvp。我们使用竖线(|)来标记文本段之间的边界。在考虑的8个任务中，只有COPA、WSC和ReCoRD要求使用PET和多个mask

**BoolQ**：是一个QA任务，每个例子由一个段落p和一个是/否问题q组成

![image-20231004102618777](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004102618777.png)

我们定义了两个表述器，分别将一个真命题映射为yes/true，另一个命题映射为no/false，共计6个pvp。

**CB和RTE**：是类似于MNLI的文本蕴涵任务，所以我们使用类似于Schick和Schütze(2021)的pvp。对于前提p和假设h，我们使用

![image-20231004102717882](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004102717882.png)

一个动词表达器将限定词映射为是，不同意映射为否，中立映射为可能。

在**COPA** 中，给定一个前提p，任务是在给定两个选项c1和c2时确定前提的因果关系。为了确定效果，我们使用以下模式

![image-20231004102850811](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004102850811.png)

对于**WiC** ，给定一个单词w和它出现的两个句子s1和s2，任务是决定w在两个句子中是否具有相同的意义。我们使用:

![image-20231004103057489](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004103057489.png)

对于前两种模式，我们用yes表示相同意思的词，用no表示其他的词;对于第三个模式，我们使用b和2。

对于**WSC**，每个例子由一个带有标记代词p和名词n的句子s组成，任务是确定p是否指n。我们将WSC视为生成任务。我们**用星号标出s中的p**，并使用以下模式:

![image-20231004103247834](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004103247834.png)

**MultiRC**是一个QA任务。给定一个段落p，一个问题q和一个答案候选人a，任务是决定a是否为q的正确答案。我们使用与BoolQ相同的动词化器，并使用类似的模式:

![image-20231004103628294](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004103628294.png)

对于**ReCoRD** ，给定一篇文章p和一个完形填空问题q，任务是决定给定的一组答案中哪一个是完形填空问题中占位符的正确替换。

### Setup

作为PET的底层LM，我们选择了ALBERTxxlarge-v2,在FewGLUE训练集上运行PET来完成所有的SuperGLUE任务。

### Results

![image-20231004104147542](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231004104147542.png)

## Conclusion

提出了一个简单但有效的PET修改，使我们能够将其用于需要预测多个令牌的任务。证明使用PET，可以实现与GPT-3类似的少样本文本分类性能