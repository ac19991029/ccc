# 减少提示和参数:使用语言模型进行简单的少样本学习

## Abstract

带有**训练示例和任务描述的提示语言模型**(LMs)被认为是最近在少样本学习中取得成功的关键。在这项工作中，我们证明了在少样本情况下精调LMs可以大大减少对快速工程的需求。事实上，可以使用空提示符，即**既不包含特定于任务的模板也不包含训练示例的提示符**，并且可以在广泛的任务中手动调优提示符，从而达到相当高的准确性。

虽然精细调优LMs确实为每个下游任务引入了新的参数，但我们表明，这种内存开销可以大大减少:当只更新0.1%的参数时，只有偏置项的精细调优可以达到与标准精细调优相当或更好的精度。总之，我们推荐使用finetuning LMs进行少样本学习，因为它对不同的提示更准确、更健壮，并且几乎可以像使用冰冻LMs一样高效。

## Introduction

少样本学习是将任务重新制定为自然语言“提示”，并使用预先训练过的语言模型来完成这些提示然而，找到这些提示是困难的:它需要对提示的措辞(也就是它的模式或模板)、是否以及如何包括训练示例、以及如何将语言模型概率转换为类预测进行一次不平凡的组合搜索。因此，提示符通常是根据人类的直觉设计的

在这项工作中，我们试图通过**识别一类简单的提示来减轻提示工程**，这些提示对于掩蔽语言模型(LMs)在许多任务中都是有效的。我们发现，当使用基于提示的微调(Schick和Schütze, 2021a;Gao等人，2021)，提示需要的优化比之前认为的要少;实际上，模式和训练示例可以完全剪切出来(例如，图1，右边)。

![image-20231001122118527](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001122118527.png)

这些空提示符——输入和[MASK]令牌的简单连接——实现了与手工编写的模式相当的准确性，同时极大地简化了提示设计:用户只需要决定标签名称(也称为措辞器)和放置[MASK]令牌的位置。空提示的有效性也挑战了一个常识，即少样本学习的成功是由于提示中存在的归纳偏见。

基于提示的微调的一个关键缺点是，它对每个新的下游任务都有很大的内存需求(图1，左)。相比之下，上下文学习(Brown et al.， 2020)允许重用大规模的lm，但它**需要显著的提示工程**。为了确定内存效率和简单的提示选择是否可以同时实现，我们进行了两个实验:(a)**使上下文学习的提示同样容易创建**，或(b)**使基于提示的微调更有效的内存**。

对于(a)，我们通过**自动调整提示符的令牌或嵌入来简化上下文学习**的提示工程。对于(b)，我们研究了更新较小参数集的轻量级微调替代方案:**BitFit、适配器和校准层。**

展示了后一种方法——使用轻量级更新进行基于提示的微调——要成功得多。特别地，只更新模型的偏置项(BitFit)可以获得比标准finetuning更有竞争力或更好的少样本精度，同时只更新0.1%的参数。

## Prompting Language Models

使用蒙面LMs进行少样本学习。

1. 一个预先训练的蒙面LM，其中T表示其标记词汇表，T∗表示所有标记序列的集合。
2. 训练输入的小集合xi∈X及其对应的标签yi∈Y。
3. 一个模式P: X→T∗，它将输入映射为包含单个[MASK]令牌的完形填空问题。此外，还有一个表达器v: Y→T，它将每个标签映射到单个词汇表令牌。我们把模式和表达符统称为提示符。

我们考虑了**构造提示符和更新蒙面LM的参数**的不同方法。表1概述了现有的提示方法以及对它们进行评估的设置。

![image-20231001122851730](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001122851730.png)

### Constructing the Prompt

#### Finetuning the LM

语境学习：最著名的少样本学习策略是使用**冻结LM** (Brown et al.， 2020)。这种策略只依赖于上下文学习(又称启动)，在这种情况下，LM通过对提示的条件反射进行学习，而不是更新参数。

基于提示的微调：不是使用冻结的LM，基于提示的微调方法**微调LM的所有参数**。对于掩码LM，这是通过**构造包含[MASK]令牌的训练示例**来完成的，并对**掩码LM进行微调**，以在该位置生成正确的语言表达器令牌。

基于提示的微调的主要优势在于，它可以实现更高的**准确性**，特别是当LM相对较小时，缺点是同一个模型不能再跨不同的任务重用，从而降低了**内存效率**。

## Experimental Setup

### Datasets and Hyperparameter Tuning

我们使用**GLUE** (Wang et al.， 2019b)和**SuperGLUE** (Wang et al.， 2019a)的以下分类数据集:**BoolQ、CB、MNLI、MRPC、QNLI、QQP、RTE和SST-2**

为了构建少样本数据集，我们从每个标签中抽样2K个例子，并使用4倍交叉验证来确定最佳超参数。在找到最佳超参数后，我们在**第一个K个例子上进行训练**，并在**第二个K个例子上尽早停止训练**。我们使用K = 16

我们从每个数据集的原始训练集中抽取示例。由于少样本学习可以是高方差的，我们用**10种不同的随机种子**抽样样本，并报告模型性能的**均值和方差**。我们使用每个数据集的原始开发集进行最终评估，并使用与每个数据集相关的标准评估指标(**准确性或F1**)。

### Masked Language Models

我们使用了由HuggingFace transformers库提供的**RoBERTa和ALBERT蒙面LMs**

### 通过# Wins比较少样本的方法

使用了一个标记为**# Wins的指标**:**给定的方法比所有其他方法执行得更好的数据集的数量**。对于给定的数据集，我们首先执行Welch’s-t检验来确定每对方法的准确性是否有显著差异，以此来计算这个度量。性能优于大多数其他方法的方法被认为是任务的“赢家”，它的#Wins加1。图2给出一个演示。

![image-20231001124654823](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001124654823.png)

## Simplifying Prompt Engineering

运行基于提示符的微调，并消除提示符的不同元素。我们考虑下列减损:

1. **手动提示(优先)**：使用来自Schick和Schütze (2021a,b)的手工编写的提示。
2. **手动提示(无工程):**通过使用直觉为每个任务手工编写一个提示来模拟标准提示设计。
3. **提示调优:**使用了来自Manual Prompt (Prior)的模式，但**随机初始化模式符号的嵌入**，并使用**基于梯度的优化**来学习它们。
4. **Null提示:**使用与Manual Prompt (Prior)相同的语言表达器，但使用的模式只包含输入字段和一个[MASK]令牌
5. **Null语言:**使用与Manual Prompt (Prior)相同的模式，但为语言表达器选择随机标记。
6. **Null提示+语言**：我们使用空提示和随机标记来表示语言表达器。

## Achieving Simplicity and Efficiency

基于提示的微调可以简化提示工程，但代价是内存效率低下——必须为每个任务学习一组新的参数。

我们将空提示和高效内存微调结合起来。我们在表2中展示了该方法以及其他最佳少样本方法的结果。

![image-20231001125048362](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001125048362.png)

我们建议使用null提示符和BitFit进行微调:它可以达到相当高的精度，设置简单，并且每个新任务的内存成本都很小。

## Conclusion and Future Work

在少样本提示中存在两种高级方法:使用冻结LM(上下文学习)和在少数训练示例上对LM进行微调(基于提示微调)。在本工作中，我们展示了基于提示的微调的两个新优点。首先，我们展示了它对**提示符的不同选择的健壮性**。事实上，有一种简单的提示符类——空提示符——可以灵活地应用于不同的任务，而不会降低与手工编写和学习的提示符相比的性能。其次，我们证明了**基于提示的微调可以提高内存效率**:仅对偏置项(BitFit)进行微调，比对所有参数进行微调，获得与微调相当或更好的精度，同时提高1000倍的内存效率。总之，在BitFit中使用空模式是一种高效、简单、且在准确性上具有竞争力的方法。