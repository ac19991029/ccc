# 微提示使预训练语言模型更适合少样本学习

代码：https://github.com/zjunlp/DART

## Abstract

大规模的预训练语言模型通过展示作为少样本学习者的卓越能力，对自然语言处理做出了重大贡献。然而，它们的有效性主要**取决于模型参数的缩放和提示的设计**，这阻碍了它们在大多数实际应用中的实现。

本文提出了一种新的可插拔、可扩展、高效的方法——**可微提示(DART)**，它可以将小的语言模型转换成更好的少样本学习者。

主要原则是**将潜在的自然语言处理任务重新构造**为一个预先训练的语言模型的任务，并通过**反向传播**对提示模板和目标标签进行差异化优化。

提出的方法可以:**插入到任何预先训练**的语言模型;扩大到广泛的**分类任务**。

## Introduction

一种新兴的微调方法，可以为较小的语言模型(LMs)配备少样本功能:通过**完成完形填字任务**，将预先训练好的LM直接作为预测器使用

在本文中，我们提出了一种新颖的**可微提示(DART)微调方法**，它是**模型无关的，参数有效的**。如图1所示，其关键思想是**利用语言模型中的几个参数**(未使用的标记)，它们作为**模板和标记**，并使用**backpropagation**在连续空间中对它们进行优化。随后，我们**引入可微提示学习**来获得优化的提示模板和标签。因为有限样本的微调可能会受到不稳定性的影响(Dodge等人(2020);Zhang et al.(2021))，我们提出了一种**共同学习模板和标签的优化算法**。进一步引入一个辅助流畅性约束对象来保证提示嵌入之间的关联。

![image-20231001133417834](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001133417834.png)

1. 提出了一种新的简单的少样本学习框架，该框架具有可插拔性、可扩展性和高效性。
2. 对15个NLP任务的系统评估表明，这种简单而有效的方法有助于所有这些任务的改进。值得注意的是，每个类只给出8个标记样本，我们提出的方法可以达到SOTA结果(全数据集)的90%性能。

## Related Work

本研究旨在开发一种新的**基于预训练语言模型的少样本学习框架**，它可以减少提示工程(包括模板和标签)和外部参数优化。此外，该方法只利用模型的非侵入性修改，可以插入任何预先训练的语言模型，并扩展到广泛的分类任务。

通过**访问少量标记示例**，少样本学习可以显著提高机器智能和实际自适应应用的学习能力。该方法与其他少样本NLP方法相适应，包括:(1)**元学习**，其中辅助任务的数量是优化的。(2)**中级培训**，通过**对数据丰富的监督任务**进行进一步的训练来补充训练前的lm。(3)**半监督学习**，该方法利用了**未标记样品**。建议的方法侧重于更实际的少样本设置(每个类的标记实例数量可以是任何变量)。

## Background

设Xin = {x1,x2，…，xL}是一个句子，其中xi是输入句子中的第i个标记，L是标记的数量。

具体来说，Xin被转换为一个**固定的令牌序列**X ~ in，然后映射到一个隐藏向量序列{hk∈Rd}。给定输入序列X ~ in = [CLS]Xin[SEP]，传统的微调方法利用在**[CLS]嵌入上的通用头层**(例如，MLP层)来预测输出类。对于基于提示的方法，设计了一**个特定于任务的模式字符串(模板T)**来诱导模型生成一个对应于给定类(标签令牌M(Y))的文本输出——我们把这两件事一起称为提示。具体来说，Xprompt包含一个[MASK]令牌，直接与MLM输入任务如下:

![image-20231001135008127](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001135008127.png)

当提示符输入到MLM时，模型可以得到候选类y∈y的概率分布p([MASK]|(Xprompt))为:

![image-20231001135128154](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001135128154.png)

其中w表示类y的WTH标签令牌。

## OUR APPROACH

对于少样本学习者来说，一个最优提示对于改进预训练语言模型是必要的。由于具有离散令牌的模板可能是次优的，不足以表示特定的class，本研究提出了**可微提示符**(differential pRompT，简称DART)，它可以减少提示符工程的需求，以提高所提方法在各个领域的适用性。

### 可微模板优化

语言标记是离散变量，通过标记搜索找到最优提示并不容易，而且很容易陷入局部极小值。

我们利用**伪标记**来构造模板，然后用**反向传播**对它们进行优化。具体来说，给定模板，T = {[T0:i]，[MASK]， [Ti+1: j]}，与传统的离散提示不同，满足[Ti]∈V，将T映射为:

![image-20231001135516864](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001135516864.png)

DART将[Ti]视为伪令牌，将模板映射如下:

![image-20231001135551449](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001135551449.png)

其中hi(0≤I≤j)为可训练参数。可微模板优化可以获得超出原始词汇表的表达模板。最后，模板hi，通过以下方式进行差异化优化:

![image-20231001135742180](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001135742180.png)

提示符嵌入的值hi必须是**相互依赖**的，而不是独立的。DART利用一个辅助流畅性约束目标来将提示嵌入相互关联，从而刺激模型专注于上下文表示学习。

### 可微标签优化

基于提示的微调需要填充一个单词，而隐藏的单词预测被映射到一个语言表达器，它生成一个类(例如，“Yes”:True)。否”:False)。

具体来说，标签Y = {Y1,Y2， ..，Ym}，与之前的方法不同，之前的方法将类类型Yi转换为数量可变的标签标记{…，v1，..，vk，…}， DART将Yj映射到一个连续的词汇空间，如下所示:

![image-20231001140202521](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001140202521.png)

其中m为模板中可训练嵌入的个数。为了避免优化任何外部参数，{h1，…hm…，hm+n}被替换为未使用的标记(例如，[unused1]或词汇表中的特殊标记)在V以生成v‘，如图1所示。

### 训练目标

由于提示模板中的伪令牌必须相互依赖，因此我们引入了一种辅助流畅性约束训练，总体而言，有两个目标:类**歧视目标和流畅性约束目标**。

Class Discrimination Object：是对句子进行分类的主要目标。如图1所示，给定(Xin,T)，我们可以生成Xprompt为:

![image-20231001140737411](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001140737411.png)

式中CE为交叉熵损失函数，LC为类分辨损失。

Fluency Constraint Object：为了确保模板令牌之间的关联，并维护从plm继承的语言理解能力，我们在MLM中利用了流畅性约束对象。如图1所示，将输入句中的一个标记随机屏蔽，并进行屏蔽语言预测。X和X’分别是原始序列和掩码序列。设xm为在x’中被屏蔽的目标令牌，g(xm|x‘，y)最大化如下:

![image-20231001141009949](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001141009949.png)

通过对LF的优化，该语言模型可以通过**模板标记之间的丰富关联**获得更好的上下文表示。我们的培训对象如下:![image-20231001141057243](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001141057243.png)

## Experiments

### datasets

我们对15个NLP任务进行了全面的研究，包括情感分析、自然语言推理、释义、句子相似度、关系抽取和事件抽取(我们只报告事件论点抽取性能)。评价由10个常用的句子分类数据集(**SST-2、MR、CR、Subj、TREC、MNLI、SNLI、QNLI、MRPC、QQP**)组成。为了进一步评估在复杂标签空间下所提方法的有效性，我们在关系抽取和事件抽取数据集上进行了实验，包括**SemEval-2010 Task 8 、tacred - re - re (Alt等人，2020)、Wiki804 、ChemProt 和ACE-20055**。

### settings

提出的模型是使用Pytorch实现的。

在LM-BFF (Gao et al.(2020))之后，我们的实验在相同的设置下进行，该设置**使用一组固定的种子**(Sseed)来衡量每个任务在5个不同的采样Dtrain中的平均表现。我们对多个超参数进行网格搜索，并为每个集合{Ds train,Ddev}， s∈Sseed选择在**Ddev**上测量的最佳结果。我们使用**AdamW**作为优化器。我们使用**RoBERTa-large** 对**分类任务**进行了实验，以与LM-BFF进行公平比较。我们对关系提取数据集使用了**未加区分的BERT-large**，但对ChemProt数据集使用了**SCIBERT**。我们遵循Soares et al.(2019)的方法，统一**使用特殊的实体标记**来突出提到的实体进行关系提取。

### main results

如表1所示，我们观察到我们的方法获得了比传统微调更好的性能，并与LM-BFF取得了可比较的结果。请注意，DART可以在**没有外部模型**(例如LM-BFF中的T5)的情况下减少提示工程，以生成易于适应其他数据集的模板。在MR数据集上，每个类只有16个训练样本，DART可以获得11.3%的提高，与LM-BFF相当，LM-BFF利用T5生成适当的提示。这些结果表明，DART能更好地激发潜在能力，使预处理语言模型成为更好的少样本学习者。我们还注意到DART比p调优产生更好的性能，这表明标签优化是有益的

![image-20231001142130800](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001142130800.png)

对于复杂标签空间的分类任务，如表2和图2(a)所示，我们观察到DART在少样本和全监督设置下都优于传统的微调方法和LM-BFF，在关系提取和事件提取数据集上都有较大的优势。

![image-20231001142347877](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001142347877.png)

当K变大时(即，从8到32)，改进缓慢衰减。

![image-20231001142521593](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001142521593.png)

### ABLATION STUDY

进行了一项消融研究，以验证该方法中组件的有效性。我们观察到，在没有任何一个模块的情况下，DART表现出性能衰减，即流畅性约束对象、可微模板或可微标签，表明所有模块都是有利的。此外，我们注意到可微标签优化对性能更敏感，并且对DART非常有益，特别是对于低资源设置。

![image-20231001142645171](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20231001142645171.png)

## Conclusion Future Work

本文提出了一种简单而有效的微调方法DART，它改进了快速学习预训练语言模型。与传统的finetuning方法相比，该方法可以在少样本情况下产生令人满意的改进。该方法还可用于其他语言模型(如BART)，并可扩展到其他任务，