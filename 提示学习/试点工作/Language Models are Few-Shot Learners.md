# Language Models are Few-Shot Learners

## Abstract

通过对大量文本进行预训练，然后对特定的任务进行微调，在许多NLP任务和基准测试上取得了实质性进展。虽然在体系结构中通常与任务无关，但这种方法仍然需要数千个或数万个示例的特定于任务的微调数据集。

我们展示了扩展语言模型极大地提高了任务无关性、少样本的性能，有时甚至达到了与先前的最先进的finetuning方法的竞争。

我们训练了GPT-3，这是一个拥有1750亿个参数的自回归语言模型，比以前任何非稀疏语言模型都要多10倍，并在少样本设置下测试了它的性能。对于所有任务，GPT-3的应用没有任何梯度更新或微调，任务和少样本演示完全通过与模型的文本交互指定。GPT-3在许多NLP数据集上都有很强的性能，包括翻译、回答问题和完形填空任务，以及一些需要即时推理或领域适应的任务，如解读单词、在句子中使用新单词或执行3位数字运算。同时，我们还确定了一些数据集，其中GPT-3的少样本学习仍然挣扎，以及一些数据集，其中GPT-3面临着与大型web语料库上的培训相关的方法问题。最后，我们发现GPT-3可以生成人类评价者难以区分的新闻文章样本。我们一般讨论这一发现和GPT-3的更广泛的社会影响。

## Introduction

在自然语言处理系统中出现了一种倾向，即采用越来越灵活和任务无关的方式进行下游迁移。首先，我们使用单词向量学习单层表征，并将其输入到特定任务结构中，然后使用具有多层表征和上下文状态的rnn来形成更强的表征

从实践的角度来看，每一个新任务都需要大量的标记示例数据集，这限制了语言模型的适用性。可能存在的有用的语言任务范围非常广泛，包括从纠正语法，生成抽象概念的例子，到评论短篇小说。对于许多这样的任务，很难收集一个大型的监督训练数据集，特别是当这个过程必须为每个新任务重复时。

其次，随着模型的可表达性和训练分布的狭窄，**利用训练数据中的虚假相关性的潜力**从根本上增加。这可能会给前训练加微调范式带来问题，在该范式中，模型被设计得很大，以便在前训练期间吸收信息，但随后在非常狭窄的任务分布上进行微调。

第三，人类不需要大型的监督数据集来学习大多数语言任务——自然语言中的简短指令

解决这些问题的一个潜在途径是元学习——在语言模型的背景下，这意味着模型在训练时发展了一套广泛的技能和模式识别能力，然后在推理时使用这些能力来快速适应或识别所需的任务

在本文中，我们**通过训练一个1750亿参数的自回归语言模型(我们称之为GPT-3)**，并测量其上下文学习能力来验证这一假设。具体来说，我们在超过24个NLP数据集上评估GPT-3，以及几个新任务，旨在测试快速适应任务，不太可能直接包含在训练集。

对于每个任务，我们在3个条件下评估GPT-3:(a)“少样本学习”，或在上下文学习中，我们允许尽可能多的演示(通常是10到100个)，(b)“一次学习”，我们只允许一个演示，(c)“零样本”学习，不允许演示，只有自然语言的指令给模型。

## Approach

### Model and Architectures

我们使用与GPT-2 [RWC+19]相同的模型和架构，包括其中描述的修改初始化、预归一化和可逆标记化，但我们在transformers的各层使用交替密集和局部带状稀疏注意模式，类似于稀疏transformers[CGRS19]。为了研究ML性能对模型大小的依赖性，我们训练了8个不同大小的模型，从1.25亿个参数到1750亿个参数，范围超过三个数量级，其中最后一个是我们称之为GPT-3的模型。

### Training Dataset

Common Crawl dataset2

我们采取了3个步骤来提高数据集的平均质量:

1. 我们下载并过滤了一个版本的CommonCrawl，基于对一系列高质量参考语料库的相似性
2. 我们在数据集内部和跨数据集的文档级别上执行模糊重复数据删除，以防止冗余，并保持我们的helout验证集的完整性，作为一种精确的过拟合度量。
3. 还在训练组合中加入了已知的高质量参考语料库，以增强CommonCrawl，增加其多样性。

![image-20230916153729740](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916153729740.png)

### Training Process

我们在训练过程中测量梯度噪声尺度，并使用它来指导我们选择批量大小

### Evaluation

对于少样本学习，我们从任务的训练集中随机抽取K个作为条件作用的例子来评估评估集中的每个例子，根据任务的不同，用1或2个新行来分隔。

对于从多个选项(多选题)中选择一个正确完成的任务，我们提供了K个上下文和正确完成的例子，然后只提供一个上下文例子，并比较每个完成的LM可能性。对于大多数任务，我们比较每个标记的可能性(对长度进行归一化)，但是对于少量的数据集(ARC, OpenBookQA，和RACE)，我们通过标准化每个完成的无条件概率，通过计算P(完成|上下文)/P(完成|答案上下文)，其中答案上下文是字符串“answer:”或“A:”，用于提示补全应该是一个答案，但其他是通用的。

对于自由形式完成的任务，我们使用与[RSR+19]参数相同的波束搜索:波束宽度为4，长度惩罚为α = 0.6。根据手头数据集的标准，我们使用F1相似度评分、BLEU或精确匹配对模型进行评分。

## Results

### Language Modeling, Cloze, and Completion Tasks

#### Language Modeling

我们计算了Penn Tree Bank (PTB) [MKM+94] [RWC+19]数据集上的零样本perplexity。由于PTB是一个传统的语言建模数据集，它没有清晰的示例分离来定义一个样本或少样本的评估，所以我们只度量零样本。

#### LAMBADA

LAMBADA数据集[PKL+16]测试了文本中长期依赖关系的建模——该模型被要求预测需要阅读一段上下文的句子的最后一个单词

![image-20230916154634514](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916154634514.png)

当以这种方式展示示例时，GPT-3在少样本设置下的精确度达到86.4%，比以前的最高水平提高了18%以上。我们观察到，少样本性能随模型尺寸的增大而显著提高。虽然这种设置将最小模型的性能降低了近20%，但对于GPT-3，它提高了10%的精度。最后，**填空方法不是有效的一次性方法**，它的性能总是比零样本设置差。也许这是因为所有的模型仍然需要几个例子来识别模式。

#### HellaSwag

HellaSwag数据集[ZHB+19]涉及为一个故事或一组指令选择最佳结局。GPT-3在单镜头设置下的准确率为78.1%，在少样本设置下的准确率为79.3%，优于微调1.5B参数语言模型[ZHR+19]的75.4%，但仍低于微调多任务模型ALUM的85.6%的总体SOTA。

#### StoryCloze

接下来，我们在StoryCloze 2016数据集[MCH+16]上评估GPT-3，这涉及到为五句长的故事选择正确的结尾句。其中GPT-3在零样本设定下为83.2%，在少样本设定下为87.7% (K = 70)。这仍然比基于BERT模型的微调SOTA低4.1% [LDL19]，但比以前的零样本结果提高了约10%。

### Closed Book Question Answering

测量GPT-3回答有关广泛事实知识的问题的能力。由于有大量的可能的查询，这个任务通常是通过使用一个信息检索系统来查找相关的文本，并结合一个模型来学习生成给定的问题和检索文本的答案来完成的。由于这个设置允许系统搜索和条件文本可能包含的答案，它被称为“开卷”。

![image-20230916155110153](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916155110153.png)

### Translation

对于GPT-2，出于容量考虑，在多语言的文档集合上使用了一个过滤器来生成只使用英语的数据集。即使使用了这种过滤，GPT-2仍然显示出了一些多语言能力的证据，并且在法语和英语之间进行翻译时表现得非常出色

现有的无监督机器翻译方法通常将对一对单语数据集的预训练与反向翻译相结合[SHB15]，以一种可控的方式连接两种语言。

### Winograd-Style Tasks

Winograd Schemas Challenge [LDM12]是NLP中的一个经典任务，它涉及到确定代词指的是哪个词，当这个代词在语法上是二义性的，但在语义上对人类来说是没有二义性的。

### Common Sense Reasoning

接下来，我们考虑三个数据集，它们试图捕捉物理或科学推理，不同于句子完成、阅读理解或广泛知识的问题回答。第一个是《PhysicalQA》(PIQA) [BZB+19]，它提出了关于物理世界如何运行的常识问题，旨在探索对世界的基础理解。

ARC [CCE+18]是一个从三年级到九年级的科学考试中收集的多项选择题的数据集。在数据集的“挑战”版本中，经过筛选，出现了一些简单的统计或信息检索方法无法正确回答的问题

![image-20230916160031402](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916160031402.png)

### Reading Comprehension

对GPT-3的阅读理解任务进行评估。我们在对话框和单一问题设置中使用了一套5个数据集，包括抽象、多项选择和基于跨度的回答格式。

GPT-3在CoQA [RCM19]的自由形式会话数据集上表现最好(在人类基线的3点以内)，在QuAC [CHI+18]的自由形式会话数据集上表现最差(比ELMo基线低13个F1)，该数据集需要建模结构化对话行为和教师-学生互动的回答跨度选择。在DROP [DWD+19]，这是一个在阅读理解环境下测试离散推理和计算能力的数据集，少样本设置的GPT-3性能优于原论文的微调BERT基线，但仍然远远低于人类性能和用符号系统增强神经网络的最新方法[RLL+19]。在SQuAD 2.0 [RJL18]中，GPT-3展示了它的少样本学习能力，与零样本设置相比提高了近10个F1(69.8)。这使得它的性能略好于原始论文中的最佳微调结果。在RACE [LXL+17]这一初高中英语考试的多项选择数据集上，GPT-3的表现相对较弱，仅能与最早使用上下文表征的作品相竞争，仍比SOTA落后45%。

### SuperGLUE

为了更好地汇总NLP任务的结果，并更系统地与BERT和RoBERTa等流行模型进行比较，我们还在一个标准化的数据集上评估GPT-3, SuperGLUE基准

![image-20230916160614837](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916160614837.png)

随着模型大小和上下文中的示例数量的增加，少样本SuperGLUE得分稳步提高

![image-20230916161251016](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916161251016.png)

### NLI

自然语言推理(NLI) [Fyo00]关注的是理解两个句子之间关系的能力。在实践中，这个任务通常被构造成一个两类或三类的分类问题，在这个问题中，模型将第二个句子从第一个句子逻辑上进行分类，是否与第一个句子相矛盾，或者可能是正确的(中立的)。

SuperGLUE包括一个NLI数据集RTE，它评估任务的二进制版本。在RTE上，只有最大版本的GPT-3在任何评估环境下的表现Synthetic and Qualitative Tasks都明显优于随机版本(56%)，但在少样本设置下，GPT-3的表现与单任务微调的BERT Large类似。

### Synthetic and Qualitative Tasks

探测GPT-3在少样本(或零拍和单拍)设置中的能力范围的一种方法是，给它一些任务，这些任务要求它执行简单的即时计算推理、识别不太可能在训练中出现的新模式，或者快速适应不寻常的任务。

### Arithmetic

为了测试GPT-3在没有特定任务训练的情况下执行简单算术操作的能力，我们开发了一个10个测试包，包括用自然语言问GPT-3一个简单的算术问题

![image-20230916162621141](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916162621141.png)

与少样本相比，一次拍摄和零样本的性能有所下降，这表明对任务的适应(或者至少是对任务的识别)对于正确执行这些计算非常重要。

### Word Scrambling and Manipulation Tasks

为了测试GPT-3从几个例子中学习新的符号操作的能力，我们设计了一个5个“字符操作”任务组。每项任务都包括给模型一个被打乱、添加或删除字符的组合扭曲的单词，并要求它恢复原来的单词。

![image-20230916163240288](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916163240288.png)

![image-20230916163317081](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916163317081.png)

### SAT Analogies

为了测试GPT-3的另一个任务，相对于典型的文本分布来说，有些不寻常，我们收集了一组374个“SAT类比”问题[TLBS03]。类比题是一种选择题，在2005年之前是SAT大学入学考试的一部分。一个典型的例子是“大胆是大胆的，就像(A)伪善是虚伪的，(b)匿名是身份，(c)悔恨是罪行，(d)有害的是结果，(e)易受影响是诱惑”。

答案是“伪善”。在这一任务中，少样本组GPT-3的得分为65.2%，单投组为59.1%，零样本组为53.7%，而大学申请者的平均得分为57%

![image-20230916165045782](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916165045782.png)

### News Article Generation

关于生成语言模型的研究定性地测试了生成语言模型生成合成“新闻文章”的能力，这种能力是通过对模型进行条件抽样，给出一个由可信的新闻故事的第一句话组成的人写提示语

为了衡量从GPT-3生成的新闻文章的质量(我们认为这可能与一般的有条件的样本生成质量有关)，我们决定衡量人类区分GPT-3生成的文章和真实文章的能力。Kreps等人[KMB20]和zeller等人[ZHR+19]也进行了类似的工作。**生成语言模型被训练来匹配由人类生成的内容的分布**，因此人类区分这两者的能力是一个潜在的重要的质量衡量标准

我们从newser.com网站任意选择了25篇文章的标题和字幕(平均长度215个单词)。然后，我们从四种语言模型中生成这些标题和字幕的补全，大小从125M到175B (GPT-3)参数(平均长度:200个单词)。对于每个模型，我们向大约80名美国参与者提供了一个测试，测试包括**这些真实的标题和字幕，后面跟着人类写的文章或由model4生成的文章**。参与者被要求选择文章是“很有可能是人类写的”、“更有可能是人类写的”、“我不知道”、“更有可能是机器写的”还是“很有可能是机器写的”。

我们选择的文章并不在模型的训练数据中，模型输出是**通过编程进行格式化和选择的，以防止人为挑选。**所有模型都使用相同的上下文来设置输出条件，并使用相同的上下文大小进行预训练，并且使用相同的文章标题和副标题作为每个模型的提示。

随着人们观察到更多的标记，人类检测模型生成文本的准确性也会提高。

### Learning and Using Novel Words

发展语言学[CB78]研究的一项任务是学习和使用新单词的能力，例如，在一个句子中只看到一个单词的定义，或者反过来从一个用法推断一个单词的意思。我们定性地测试GPT-3实现前者的能力

### Correcting English Grammar

另一项非常适合少样本学习的任务是纠正英语语法。

## Measuring and Preventing Memorization Of Benchmarks

我们的训练数据集来自互联网，所以我们的模型可能是在一些**基准测试集上训练的**。准确地从互联网规模的数据集中检测测试污染是一个新的研究领域，没有建立最佳实践。

GPT-3的运作方式有些不同。一方面，**数据集和模型的大小**比GPT-2使用的要大两个数量级，并且包含了大量的Common Crawl，**增加了污染和记忆的可能性**。另一方面，正是由于数据量大，即使是GPT-3 175B也**没有对其训练集进行大量过拟合**，这是相对于对其进行去重操作的helout验证集而言的(图4.1)。因此，我们预计污染很可能是频繁的，但其影响可能没有担心的那么大。

我们最初试图通过**主动搜索并试图删除我们的训练数据与本文中研究的所有基准的开发和测试集之间的任何重叠来解决污染问题。**

不幸的是，一个错误导致训练数据中所有检测到的重叠部分仅被部分删除。由于培训成本的原因，对该模型进行再培训是不可行的。为了解决这个问题，我们详细研究了**剩余检测到的重叠如何影响结果**。

对于每个基准测试，我们都生成了一个“干净”的版本，它删除了所有可能泄露的示例

我们在这些干净的基准上评估GPT-3，并与原始分数进行比较。如果clean子集上的分数与整个数据集上的分数相似，这表明**即使存在污染，也不会对报告的结果产生显著影响**。

![image-20230916171119870](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916171119870.png)

## Limitations

GPT-3在数量和质量上有了很大的改进，特别是与它的直接前身GPT-2相比，它在**文本合成和几个NLP任务方面仍然存在明显的不足。**

在文本合成方面，尽管整体质量很高，GPT-3样本有时仍然在文档级别上重复自己的语义，在足够长的段落时开始失去连贯性，自相矛盾，偶尔包含不符合逻辑的句子或段落。

在离散语言任务领域，我们已经非正式地注意到GPT-3似乎在“常识物理”方面有特殊的困难，尽管在测试该领域的一些数据集(如PIQA [BZB+19])上做得很好。具体来说，GPT-3很难回答“如果我把奶酪放进冰箱，它会融化吗?”

GPT-3有一些结构和算法的限制，这可以解释上面的一些问题。我们专注于在**自回归语言模型中探索上下文学习行为**，因为用这个模型类进行抽样和计算可能性都是直接的。

本文中描述的一般方法的一个更根本的限制——扩大任何类似lm的模型，无论是自回归的还是双向的——是它最终可能会(或已经会)遇到训练前目标的限制。

语言模型普遍存在的另一个限制是**在训练前的低样本效率。**提高训练前样本的效率是未来工作的一个重要方向，可能来自于在物理世界中提供额外信息的基础，或来自于算法的改进。

在GPT-3中，与少样本学习相关的一个限制，或者至少是不确定性，是**关于少样本学习是否真的在推断时间“从零开始”地学习新任务，还是仅仅识别和识别它在培训期间学习的任务的模糊性**。

与GPT-3尺度的模型相关的一个限制，无论目标函数或算法如何，是**它们既昂贵又不方便进行推理**，这可能对当前形式的该尺度模型的实际适用性提出了挑战。解决这个问题的一个可能的未来方向是**对大型模型进行提炼**,**将其细化到特定任务的可管理规模**

GPT-3与大多数深度学习系统有一些共同的局限性——**它的决定不容易解释**，它在对新输入的预测上不一定经过良好的**校准**，因为在标准基准上，它的表现差异比人类的要大得多，**而且它保留了训练数据的偏差**。最后一个问题——数据中的偏见可能导致模型产生**刻板印象或偏见的内容**

## Broader Impacts

语言模型对社会有广泛的有益应用，包括代码和编写自动完成、语法辅助、游戏叙事生成、改进搜索引擎响应和回答问题。

我们关注改进的语言模型的潜在危害，不是因为我们认为危害必然更大，而是为了**刺激研究和减轻它们的努力**。这种语言模型的广泛影响是多方面的。我们关注两个主要问题:**故意滥用语言模型**的可能性，以及**偏见、公平和模型**中的代表性问题。

### Misuse of Language Models

恶意使用语言模型可能有点难以预料，因为它们通常涉及在一个非常不同的环境中或为了一个与研究人员预期不同的目的而重新使用语言模型。为了帮助解决这一问题，我们可以从传统安全风险评估框架的角度进行思考，该框架概述了关键步骤，如识别威胁和潜在影响，评估可能性，并将风险确定为可能性和影响的组合[

讨论了三个因素:潜在的滥用应用程序、威胁参与者和外部激励结构。

#### Potential Misuse Applications

任何依赖于生成文本的对社会有害的活动都可以通过强大的语言模型加以扩充。例子包括虚假信息、垃圾邮件、网络钓鱼、滥用法律和政府程序、欺骗性学术论文写作和社会工程借口。许多此类应用程序阻碍了人类编写足够高质量的文本。能够**产生高质量文本生成的语言模型**可以降低现有的执行这些活动的障碍，并提高其有效性

#### Threat Actor Analysis

威胁行动者可以按技能和资源级别组织，从可能构建恶意产品的低技能或中等技能和资源的行动者到“高级持续威胁”(apt):具有长期议程的高技能和资源充足的(如国家支持的)团体

#### External Incentive Structures

每个威胁参与者组也有一组战术、技术和程序(TTPs)，它们依靠这些来完成它们的议程。ttp受经济因素的影响，如可扩展性和易于部署;网络钓鱼在所有群体中都非常流行，因为它提供了一种低成本、低成本、高收益的部署恶意软件和窃取登录凭证的方法。使用语言模型来扩充现有的ttp可能会导致更低的部署成本。

**易用性**是另一个重要的激励因素。拥有稳定的基础设施对采用ttp有很大的影响。然而，语言模型的输出是随机的，尽管开发人员可以限制这些输出(例如使用top-k截断)，但如果没有人类的反馈，他们就不能始终如一地执行。

我们怀疑人工智能研究人员最终将开发出足够一致和可操纵的语言模型，从而对恶意行动者产生更大的兴趣。我们希望这将为更广泛的研究界带来挑战，并希望通过结合缓解研究、原型设计以及与其他技术开发人员协调来解决这一问题。

### Fairness, Bias, and Representation

训练数据中的偏见可能会导致模型生成刻板或有偏见的内容。

总的来说，我们的分析表明，互联网训练的模型存在互联网规模的偏差;模型倾向于反映其训练数据中呈现的刻板印象。