# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

## 代码:https://github.com/google-research/text-to-text-transfer-transformer

## Abstract

## Introduction

我们工作的基本思想是**将每一个文本处理问题视为“文本到文本”的问题**，即**以文本作为输入，以生成新的文本作为输出**。这种方法受到了之前统一的NLP任务框架的启发，包括将所有文本问题转换为问答(McCann等人，2018)、语言建模(Radford等人，2019)或跨度提取Keskar等人(2019b)任务。至关重要的是，文本到文本框架允许我们直接将相同的模型、目标、训练过程和解码过程应用于我们考虑的每一个任务。我们通过评估各种基于英语的NLP问题的表现来利用这种灵活性，包括问题回答、文档摘要和情感分类等。通过这种统一的方法，我们可以比较不同迁移学习目标、未标记数据集和其他因素的有效性，同时通过扩展超出之前考虑的模型和数据集，探索NLP迁移学习的局限性。

我们的目标不是提出新的方法，而是就该领域的现状提供一个全面的观点。因此，我们的工作主要包括**对现有技术的调查、探索和经验比较**。我们还通过扩大我们系统研究(训练模型多达110亿个参数)的见解，探索当前方法的局限性，以在我们考虑的许多任务中获得最先进的结果。为了在这种规模上进行实验，我们引入了“**巨大的语料库”(C4)**，这是一个由数百gb的从网络上抓取的干净的英语文本组成的数据集。认识到迁移学习的主要用途是在数据稀缺的环境中利用预训练模型的可能性，我们发布了我们的代码、数据集和预训练模型

## Setup

在介绍我们大规模实证研究的结果之前，我们回顾了理解我们的结果所需的必要背景主题，包括Transformer模型体系结构和我们评估的下游任务。我们还介绍了**我们将每个问题视为文本到文本任务的方法**，并描述了我们的“巨大语料库”(C4)，这是我们作为未标记文本数据源创建的基于公共爬行的数据集。我们**将我们的模型和框架称为“文本到文本传输转换器”(T5)。**

### Model

我们研究的所有模型都基于Transformer体系结构。

Transformer的主要组成部分是**自我注意**(Cheng等人，2016)。自我注意是注意的一种变体(Graves, 2013;Bahdanau等人，2015)**通过用序列其余部分的加权平均替换每个元素来处理序列。**最初的Transformer由编码器-解码器架构组成，**用于序列到序列**.最近，使用由单个Transformer层堆栈组成的模型也变得很普遍，使用各种形式的自我关注来生成适合于语言建模的架构(Radford等人，2018;al - rou等人，2019)或分类和跨度预测任务

我们的编码器-解码器Transformer实现紧跟其最初提出的形式(Vaswani等人，2017)。首先，**将标记的输入序列映射到嵌入序列，然后将嵌入序列传递到编码器。编码器由一堆“块”组成，每个“块”由两个子组件组成:一个自注意层，后面跟着一个小的前馈网络。对每个子组件的输入应用层归一化**

我们使用一种简化版本的层标准化，其中激活只被缩放，没有附加偏倚应用。经过层归一化后，剩余的跳过连接(He et al.， 2016)将每个子组件的输入添加到其输出中。

Dropout (Srivastava et al.， 2014)应用于前馈网络、skip连接、注意权值以及整个堆栈的输入和输出。解码器在结构上与编码器相似，除了它在每个自注意层之后包括一个标准的注意机制，该自注意层负责编码器的输出。解码器中的自我注意机制也使用了一种自回归或因果自我注意的形式，它只允许模型注意过去的输出。最终的解码器块的输出被送入具有软最大输出的密集层，该输出的权值与输入嵌入矩阵共享。Transformer中的所有注意力机制都被分割成独立的“头”，它们的输出在进一步处理之前被连接起来。

**自注意是顺序独立的**(即它是集合上的一个操作)，通常会向Transformer提供一个显式的位置信号。虽然最初的Transformer使用正弦位置信号或学习的位置嵌入，但最近越来越普遍地使用相对位置嵌入

相对位置嵌入不是对每个位置使用固定的嵌入，而是**根据自注意机制中被比较的“键”和“查询”之间的偏移量产生不同的学习嵌入**。我们使用一种简化的位置嵌入形式，其中**每个“嵌入”只是一个标量**，添加到用于计算注意力权重的相应logit中。为了提高效率，我们还**在模型的所有层中共享位置嵌入参数**，尽管在给定的层中，每个注意力头使用不同的学习位置嵌入。

通常，要学习固定数量的嵌入，每个嵌入对应一个可能的键查询偏移量范围。在这项工作中，我们对所有模型使用了32个嵌入，其范围以对数的方式增加到128的偏移量，超过这个偏移量，我们将所有相对位置分配给相同的嵌入。

### The Colossal Clean Crawled Corpus

以往许多关于NLP迁移学习的工作都是利用大量无标记数据集进行无监督学习。在这篇论文中，我们感兴趣的**测量质量，特征，和大小的影响*****，这一未标记的数据**。为了生成满足我们需求的数据集，我们利用**Common Crawl作为从网络抓取文本的来源。**

Common Crawl是一个公开的网络存档，它通过**从HTML文件中删除标记和其他非文本内容来提供“网络提取文本”。**不幸的是，大多数生成的文本不是自然语言。相反，它主要包含**废话或样板文本，如菜单、错误消息或重复文本。**此外，**大量的文本包含的内容对我们所考虑的任何任务都不太可能有帮助**(冒犯性语言、占位符文本、源代码等)。为了解决这些问题，我们使用以下启发式方法来清理Common Crawl的web提取文本:

1. 我们只保留以结束标点符号(即句号、感叹号、问号或结束引号)结束的行。
2. 我们丢弃任何少于5句话的页面，只保留包含至少3个单词的行。
3. 我们删除了所有包含“肮脏、顽皮、淫秽或其他不良词汇列表”中的任何单词的页面。
4. 许多被删除的页面都包含警告，说明应该启用Javascript，所以我们删除了任何带有Javascript这个词的行。
5. 有些页面有占位符“lorem ipsum”文本;我们删除了任何出现“lorem ipsum”的页面。
6. 一些页面无意中包含了代码。由于大括号“{”出现在许多编程语言中(如Javascript，在web上广泛使用)，但不出现在自然文本中，所以我们删除了任何包含大括号的页面。
7. 为了去重数据集，我们丢弃了数据集中出现不止一次的任何三句话跨度中的一个。

我们的大多数下游任务都集中在英语文本上，所以我们使用**langdetect**以至少0.99的概率过滤掉没有被分类为英语的任何页面。

### Downstream Tasks

本文的目标是测量一般语言学习能力。因此，我们在不同的基准集上研究下游性能，包括机器翻译、问题回答、抽象摘要和文本分类。

具体来说，我们测量**GLUE和SuperGLUE文本分类元基准的性能;CNN/每日邮报摘要;小组问答;以及WMT从英语到德语、法语和罗马尼亚语的翻译。**

**GLUE** (Wang et al.， 2018)和**SuperGLUE** (Wang et al.， 2019b)各自包含一组文本分类任务，用于测试一般语言理解能力.使用由GLUE和SuperGLUE基准测试分发的数据集。为了简单起见，在进行微调时，**通过连接所有组成数据集**，我们将GLUE基准测试中的所有任务(SuperGLUE也是如此)视为**单个任务**。

**CNN/每日邮报**(Hermann et al.， 2015)的数据集被引入作为一个**问答任务**，但被Nallapati et al.(2016)改编为文本摘要;我们使用See等人(2017)的非匿名版本作为**抽象摘要任务**。SQuAD (Rajpurkar等人，2016)是一个**常见的问答基准。**

在我们的实验中，**向模型输入问题及其上下文，并要求它逐令牌生成答案**。

### Input and Output Format

我们将我们考虑的所有任务**强制转换为“文本到文本”的格式**--也就是说，在这样的任务中，模型被提供一些文本作为上下文或条件，然后被要求产生一些输出文本。为了指定模型应该执行哪个任务，我们在**将原始输入序列提供给模型之前，向其添加一个特定于任务的（文本）前缀。**

除了STS-B之外，我们可以直接将**所有考虑的任务转换为文本到文本的格式**，STS-B是一个回归任务，其目标是**预测相似性得分在1到5之间**。我们发现这些分数大多以0.2为增量进行注解，所以我们简单地将任何分数四舍五入到最接近的增量0.2，并将结果转换为数字的文字字符串表示(例如，浮点值2.57将被映射到字符串“2.6”)。

另外，我们还将Winograd任务(来自GLUE的WNLI，来自SuperGLUE的WSC，以及我们添加到SuperGLUE的DPR数据集)转换成更简单的格式，更适合文本到文本框架。

Winograd任务中的例子包括一段包含有歧义代词的文章，该代词可以指代文章中多个名词短语。例如，这一段可能是“市议会拒绝了示威者的许可，因为他们害怕暴力。，其中包含了模棱两可的代词“他们”，可以指“市议员”或“示威者”。我们将WNLI、WSC和DPR任务**定义为文本到文本的问题**，方法是**突出文本段落中不明确的代词，并要求模型预测它所指代的名词。**上面提到的例子可以转换成“市议会拒绝示威者的许可，因为他们害怕暴力。”，然后训练该模型来预测目标文本“市议员”。

对于WSC，示例包含段落、歧义代词、候选名词和True/False标签，反映候选人是否匹配代词(忽略任何文章)。**我们只训练带有“真”标签的例子，因为我们不知道带有“假”标签的例子的正确名词目标。**如果模型输出中的单词是候选名词短语中的单词的子集(反之亦然)，我们就分配一个“True”标签，否则就分配一个“False”标签。

WNLI的训练和验证集与WSC的训练集有很大的重叠。

## Experiments

我们通过制定一个**合理的基线**并每次改变设置的一个方面来系统地研究这些贡献。

我们的目标是在保持尽可能多的固定因素的同时，对不同任务的不同方法进行比较。为了实现这一目标，在某些情况下，我们并不完全复制现有的方法。例如，像BERT (Devlin et al.， 2018)这样的“纯编码器”模型被设计为**为每个输入标记生成一个预测，或为整个输入序列生成一个预测**。这使得它们适用于分类或跨度预测任务，但不适用于生成性任务，如翻译或抽象摘要。

考虑一个类似于第3.3节中的BERT的“**掩蔽语言建模**”目标，我们考虑一个类似于第3.2节中的BERT的文本分类任务的模型体系结构。

### Baseline

我们的基线目标是反映典型的、现代的实践。我们使用一个简单的去噪目标预先训练一个标准Transformer，然后分别对每个下游任务进行微调。

#### Model

对于我们的模型，我们使用了Vaswani等人(2017)提出的标准编码器-解码器转换器。

基线模型是这样设计的，编码器和解码器在大小和配置上都类似于“BERTBASE”堆栈.具体来说，**编码器和解码器都由12个块组成(每个块包括自注意、可选的编码器-解码器注意和一个前馈网络)。**

每个块的前馈网络由输出维数为dff = 3072的密集层、ReLU非线性层和密集层组成。所有注意机制的“键”和“值”矩阵的内维均为dkv = 64，所有注意机制都有12个头。所有其他子层和嵌入的维数都是dmodel = 768。

**基线模型包含两层堆栈，而不是一层。为了进行正则化，我们在模型中所有应用dropout的地方使用了0.1的dropout概率。**

#### Training

所有任务都被定义为文本到文本的任务。这允许我们总是使用标准的最大似然来训练，即使用**teacher forcing(**Williams和Zipser, 1989)和**交叉熵损失**。为了优化，我们使用**AdaFactor** (Shazeer和Stern, 2018)。在测试时，我们使用**贪心解码**(即在每个时间步上选择最高概率的logit)。

在进行微调之前，我们对每个模型进行2 19= 524,288步的预训练。我们使用的最大序列长度为512，批量大小为128个序列。只要可能，我们将多个序列“打包”到batch10的每个条目中，这样我们的批次大约包含2 16 = 65,536个令牌。

在预训练期间，我们使用“**平方根反比**”学习速度计划:1/根号max(n, k)，其中n为当前训练迭代，k为热身步数(在我们所有的实验中设置为104)。这将前104步的学习速率设定为0.01，然后以指数形式衰减学习速率，直到训练前结束。

我们的模型对所有任务进行了218 = 262,144步的微调。选择这个值是为了在高资源任务(即具有大数据集的任务)和低资源任务(较小的数据集)之间进行权衡，前者受益于额外的微调，后者过拟合很快。

#### Vocabulary

我们使用sentencpiece (Kudo和Richardson, 2018)将文本编码为WordPiece标记

将C4中使用的Common Crawl刮擦页分类为**德语、法语和罗马尼亚语**。然后，我们在10部分英语C4数据和1部分分类为德语、法语或罗马尼亚语的数据的混合上训练我们的句子片段模型。该词汇表在模型的输入和输出之间共享。

#### Unsupervised Objective

利用未标记的数据对我们的模型进行预训练**需要一个不需要标记的目标**.但(松散地说)**向模型传授将在下游任务中有用的可概括的知识。**

去噪”目标(Devlin等人，2018;(也称为“蒙面语言建模”)产生了更好的性能，因此它们迅速成为标准。在去噪目标中，训练模型预测输入中缺失或损坏的标记。

我们设计了一个**随机抽样**的目标，然后**剔除输入序列中15%的令牌**。所有被退出的令牌的连续时间段都**被一个单独的哨兵令牌所取代**。为每个哨兵令牌分配一个令牌ID，该令牌ID对序列是唯一的。哨兵id是添加到词汇表中的特殊标记，不对应于任何单词。然后，**目标对应于所有被删除的标记范围**，这些标记由**输入序列中使用的相同标记和一个最终标记**来标记目标序列的结束。我们选择隐藏连续区间的令牌，并只预测掉出的令牌，是为了减少训练前的计算成本。

#### Baseline Performance

使用上面描述的基线实验过程来展示结果，以了解在我们的下游任务套件上期望什么样的性能。

1. 我们**从头开始训练我们的基线模型10次**(即使用不同的随机初始化和数据集洗牌)，并假设这些基本模型运行的方差也适用于每个实验变量。
2. 对于GLUE和SuperGLUE，我们在标题“GLUE”和“SGLUE”下报告所有子任务的平均分数(按照官方基准的规定)。
3. 对于所有的翻译任务，我们报告了SacreBLEU v1.3.0 (Post, 2018)提供的带有“exp”平滑和“intl”标记化的BLEU分数(Papineni等人，2002)。我们将WMT英语到德语、英语到法语、英语到罗马尼亚语的分数分别称为EnDe、EnFr和EnRo。
4. 对于CNN/Daily Mail，我们发现模型在ROUGE-1-F、ROUGE-2-F和ROUGE-L-F指标上的表现(Lin, 2004)高度相关，所以我们在“CNNDM”标题下单独报告ROUGE-2-F得分。
5. 对于SQuAD，我们发现“精确匹配”和“F1”分数的表现高度相关，所以我们只报告“精确匹配”分数。

我们的结果表都经过了格式化，以便每一行都对应于一个特定的实验配置，其中的列给出了每个基准测试的分数。我们将在大多数表中包含基线配置的平均性能。无论基线配置出现在哪里，我们都将它标记为☆(如表1的第一行所示)。在给定的实验中，我们还将在最大(最好)两个标准差范围内的任何分数用黑体表示。

![image-20230919165148866](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230919165148866.png)

### Architectures

#### Model Structures

不同架构的一个主要区别因素是模型中不同的注意力机制所使用的“mask”。

Transformer中的自注意操作接受一个序列作为输入，并输出一个相同长度的新序列。输出序列的每个条目都是通过计算输入序列条目的加权平均而产生的。即yi表示输出序列的第i个元素，xj表示输入序列的第j个元素。

Yi计算为Σj wi,jxj，其中wi,j是自注意机制产生的标量权重，它是xi和xj的函数。然后，注意掩码用于将某些权重归零，以约束输入中的哪些条目可以在给定的输出时间步长上被关注。

第一个模型结构是一个编码器-解码器转换器，它由两层堆栈组成:编码器，提供一个输入序列，解码器，产生一个新的输出序列。这个体系结构变体的示意图显示在图4的左面板中。

![image-20230922100158314](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230922100158314.png)

编码器使用“完全可见”的注意掩码。**完全可见掩蔽允许一种自注意机制在产生输出的每一项时注意输入的任何一项。**

BERT (Devlin et al.， 2018)也使用了完全可见的屏蔽模式，并在输入中添加了一个特殊的“分类”标记。然后使用与分类令牌相对应的时间步上的BERT的输出对输入序列进行分类预测。

在Transformer的解码器中，自我注意操作使用了一种“因果”掩蔽模式。当产生输出序列的第i个条目时，因果掩蔽会阻止模型处理第j >i个输入序列的第j个条目。这在训练期间使用，以便模型在产生输出时不能“看到未来”。

编码器-解码器转换器中的**解码器用于自回归产生输出序列**。也就是说，在每个输出时间步中，从模型的预测分布中采样一个令牌，然后将该样本反馈到模型中，以产生对下一个输出时间步的预测，以此类推。

Transformer解码器(不带编码器)可以用作语言模型(LM)，即仅为下一步预测而训练的模型(Liu等人，2018;Radford等人，2018;al - rou等，2019)。这构成了我们考虑的第二个模型结构。

语言模型通常用于**压缩或序列生成**(Graves, 2013)。但是，它们**也可以在文本到文本框架中使用**，只需将输入和目标连接起来

在文本到文本的设置中使用语言模型的一个基本和经常被引用的缺点是，因果掩蔽迫使模型对输入序列的第i项的表示只依赖于直到i的项。

在基于transformer的语言模型中，只需改变掩蔽模式就可以避免这个问题。我们在序列的前缀部分使用完全可见的遮罩，而不是使用因果遮罩。

#### Comparing Different Model Structures

为了提供一种合理的比较方法，我们为我们的编码器-解码器模型考虑了多种配置。我们将bertbase大小的层堆栈中的层数和参数分别称为L和P。我们将使用M表示L+L层编码器-解码器模型或L层解码器模型处理给定输入目标对所需的浮点数。总的来说，我们将比较:

1. 编码器层为L层，解码器层为L层的编码器-解码器模型。该模型具有2P参数和M个FLOPs的计算量
2. 一个等效的模型，但参数在编码器和解码器之间共享，导致P个参数和M-FLOP的计算成本。
3. 在编码器和解码器中各有L/2层的编码器-解码器模型，给出了P个参数和M/2- flop的代价。
4. 一个只有解码器的语言模型，有L层和P个参数，计算代价为M个浮点数。
5. 只有解码器的前缀LM具有相同的体系结构(因此具有相同数量的参数和计算成本)，但对输入具有完全可见的自我关注。

#### Objectives

作为一个无监督目标，我们将考虑一个**基本的语言建模目标以及基线去噪目标**。我们包括了语言建模目标，因为它曾被用作培训前的目标(Dai和Le, 2015;Ramachandran等人，2016;霍华德和罗德，2018年;Radford等人，2018;Peters等人，2018)，以及它对我们所考虑的语言模型架构的自然适合。

对于在进行预测之前摄取前缀的模型(编码器-解码器模型和前缀LM)，我们从**未标记的数据集中采样一段文本**，并**选择一个随机点将其分割为前缀和目标部分**。对于标准语言模型，我们训练模型来**预测从开始到结束的整个跨度**。我们的无监督去噪目标是为文本到文本模型设计的;为了适应语言模型，我们将输入和目标连接起来

#### Results

表2显示了我们所比较的每个体系结构的得分。在所有任务中，具有去噪目标的编解码器结构表现最佳。这种变体具有最高的参数计数(2P)，但与p参数解码器模型相同的计算成本。令人惊讶的是，我们发现在编码器和解码器之间共享参数的效果几乎一样好。相比之下，将编码器和解码器堆栈中的层数减半会显著降低性能。

![image-20230922103224709](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230922103224709.png)

跨Transformer块共享参数可以有效降低总参数数量，同时不牺牲太多性能。共享参数编码器-解码器的性能优于仅使用解码器的前缀LM，这表明添加显式的编码器-解码器注意是有益的。最后，我们确认了一个被广泛接受的概念，即**与语言建模目标相比，使用降噪目标总是能获得更好的下游任务性能。**

### Unsupervised Objectives

无监督目标的选择是至关重要的，因为它**提供了一种机制**，通过这种机制，**模型可以获得通用知识，并应用于下游任务。**

总的来说，我们的所有目标都需要**一个令牌id序列**，该序列**对应于未标记文本数据集中经过标记的文本范围**。**处理令牌序列以生成(已损坏的)输入序列和相应的目标**。然后，以最大似然对模型进行训练，对目标序列进行预测。

#### Disparate High-Level Approaches

我们比较了三种受常用目标启发但在方法上有显著差异的技术。

首先，我们包括一个基本的“前缀语言建模”目标，这种技术**将一段文本分割成两个组件，一个用作编码器的输入，另一个用作解码器预测的目标序列。**

其次，我们考虑一个受BERT中使用的“蒙面语言建模”(MLM)目标启发的目标(Devlin et al.， 2018)。**mask采取了一个跨度的文本和损坏15%的令牌**。90%的损坏标记被替换为**一个特殊的掩码标记**，10%被**替换为一个随机标记**。由于BERT是一个纯编码器模型，它在预训练期间的目标是**在编码器的输出上重建mask令牌**。在编码器-解码器的情况下，我们简单地使用整个未损坏的序列作为目标。注意，这与我们的基线目标不同，它只使用已损坏的标记作为目标

最后，我们还考虑了一个基本的去噪目标。这种方法**获取一个标记序列，对其进行洗牌，然后使用原始的洗牌序列作为目标。**

![image-20230922104337811](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230922104337811.png)

总的来说，我们发现bert风格的目标表现最好，尽管前缀语言建模目标在翻译任务上取得了类似的性能。事实上，BERT目标的动机是超越基于语言模型的前训练。与前缀语言建模和bert风格的目标相比，去噪目标的性能要差得多。

![image-20230922105501949](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230922105501949.png)

#### Simplifying the BERT Objective

我们现在将重点探讨对**bert风格去噪目标的修改**。这个目标最初是作为一种用于分类和跨度预测训练的编码器模型的前训练技术提出的。因此，可以对其进行修改，使其在我们的编码器-解码器文本到文本设置中执行得更好或更高效。

首先，我们考虑bert风格目标的一个简单变体，其中不包括随机令牌交换步骤。结果的目标只是用掩码标记替换输入中的15%的标记，然后训练模型来重建原始的未损坏序列。Song等人(2019)使用了一个类似的掩蔽目标，它被称为“MASS”，所以我们称这种变体为“MASS风格”的目标。

其次，我们感兴趣的是，是否有可能避免预测整个未损坏的文本范围，因为这需要对解码器中的长序列进行自我关注。

我们考虑两种策略来实现这一点:首先，**不是用掩码令牌替换每个损坏的令牌**，而是**用唯一的掩码令牌替换**每个连续的损坏令牌跨度的**整体**。然后，目标序列**成为“已损坏”的跨度的串联**，每个跨度的前缀是掩码令牌，用于在输入中替换它。

我们还考虑了一种变体，在这种变体中，我们简单地从输入序列中完全删除已损坏的令牌，并让模型按顺序重新构建已删除的令牌。表3的第5行和第6行显示了这些方法的示例。

原始bert风格的目标与这三个备选方案的实证比较如表5所示。我们发现，在我们的设置中，所有这些变量的表现都很相似。唯一的例外是，完全删除损坏的令牌会对GLUE分数产生微小的改善

![image-20230922111832992](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230922111832992.png)

在SuperGLUE上，删除token完全比用哨兵token替换它们更糟糕。两种不需要预测完整原始序列的变体(“替换损坏跨度”和“掉落损坏跨度”)都具有潜在的吸引力，因为它们使目标序列更短，从而使训练更快。

#### Varying the Corruption Rate

们已经损坏了BERT中使用的15%的令牌(Devlin et al.， 2018)。同样，由于我们的文本到文本框架不同于BERT的框架，我们有兴趣看看不同的损失率是否更适合我们。我们比较了表6中10%、15%、25%和50%的损失率。总的来说，我们发现损失率对模型的性能有有限的影响。唯一的例外是，我们认为最大的损失率(50%)会导致GLUE和SQuAD的性能显著下降。使用更大的损失率也导致更长的目标，这可能会减慢培训。根据这些结果和BERT设定的历史先例，我们将使用15%的损失率。

#### Corrupting Spans

我们现在转向**通过预测较短的目标来加速训练的目标**。

到目前为止，我们使用的方法为**每个输入令牌做出标识，决定是否破坏它**。当**多个连续的令牌被破坏时**，它们**被视为一个“跨度”，一个唯一的掩码令牌被用来替换整个跨度**。用单个标记替换整个跨度会导致未标记的文本数据被处理成更短的序列。由于我们使用的是id破坏策略，因此并不总是连续出现大量破坏令牌的情况。因此，我们可以通过特定地破坏令牌跨度而不是以i.i.d.的方式破坏单个令牌来获得额外的加速。

为了测试这个想法，我们考虑了一个目标，该目标**专门破坏连续的、随机间隔的令牌范围**。这个目标可以通过破坏令牌的比例和破坏广度的总数来参数化。然后随机选择跨度长度以满足这些指定的参数。

如果我们正在处理一个包含500个令牌的序列，并且我们已经指定了15%的令牌应该被破坏，并且应该有25个总跨度，那么被破坏的令牌的总数将是500×0.15 = 75，平均跨度长度将是75/25 = 3。注意，给定原始序列长度和损坏率，我们可以等效地用平均跨度长度或跨度的总数来参数化这个目标。

我们将表7中的span-corruption目标与i.i.d-corruption目标进行比较。我们在所有情况下都使用15%的腐败率，并使用2、3、5和10的平均跨度长度进行比较。同样，我们发现这些目标之间的差异有限，尽管平均跨度长度为10的版本在某些情况下略低于其他值。我们还特别发现，在大多数非翻译基准测试中，使用3的平均跨度长度稍微(但明显)优于i.i.d.目标。幸运的是，跨度腐蚀目标在训练过程中也提供了一些加速，因为跨度腐蚀平均产生更短的序列。

![image-20230922112700065](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230922112700065.png)

#### Discussion

图5显示了我们在探索无监督目标时所做选择的流程图。总的来说，我们观察到的最显著的性能差异是去噪目标优于语言建模和训练前的去噪目标。在我们所探索的去噪目标的许多变体中，我们没有观察到显著的差异。然而，不同的目标(或目标的参数化)会导致不同的序列长度，从而导致不同的训练速度。

![image-20230922112826503](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230922112826503.png)

### Pre-training Data set

像无监督目标一样，训练前数据集本身是迁移学习管道的关键组成部分。然而，与目标和基准不同，新的训练前数据集通常不会被视为独立的重要贡献，而且通常不会与训练前的模型和代码一起发布。相反，它们通常是在呈现新方法或模型的过程中引入的。因此，不同的训练前数据集的比较相对较少，也缺乏用于训练前的“标准”数据集。

我们比较C4数据集的变体和其他潜在的训练前数据来源。我们发布了所有**我们认为是TensorFlow数据集一部分的C4数据集变体**

#### Unlabeled Data Sets

在创建C4时，我们开发了各种启发式方法来过滤从Common Crawl中提取的web文本

除了将其与其他过滤方法和常用的训练前数据集进行比较之外，我们还对测量这种过滤是否能提高下游任务的性能感兴趣。为此，我们对以下数据集进行预训练后，比较我们的基线模型的性能:

1. C4：作为基线，我们首先考虑对2.2节中所述的建议的未标记数据集进行预训练。
2. Unfiltered C4：为了测量我们在创建C4时使用的启发式过滤的效果(重复数据删除、删除不良词汇、只保留句子等)，我们还生成了一个替代的C4版本，该版本放弃了这种过滤。
3. RealNews-like：使用了从新闻网站提取的文本数据，与这种方法相比，我们通过额外过滤C4来生成另一个未标记的数据集，以仅包含“RealNews”数据集中使用的一个域的内容
4. WebText-like：WebText数据集(Radford et al.， 2019)只使用提交到内容聚合网站Reddit并获得至少3分的网页内容。
5. Wikipedia：维基百科网站由数百万篇百科全书文章组成。使用TensorFlow数据集中的英文维基百科文本数据，它省略了文章中的任何标记或引用部分。
6. Wikipedia + Toronto Books Corpus：结合了来自维基百科和多伦多图书语料库(TBC)的数据(Zhu et al.， 2015)。TBC包含从电子书中提取的文本，它代表了自然语言的一个不同领域。

![image-20230922113757613](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230922113757613.png)

对每个数据集进行预训练后的结果如表8所示。第一个明显的发现是，从C4中删除启发式过滤会均匀地降低性能，并使未过滤的变体在每个任务中执行得最差。除此之外，我们还发现，在某些情况下，具有更多约束域的预训练数据集比不同的C4数据集表现更好。

### Training StrFine-tuning Methodsategy

#### Fine-tuning Methods

对模型的所有参数进行微调可能导致次优结果，特别是在低资源任务上。我们关注两种可选的微调方法，它们只更新编码器-解码器模型参数的一个子集。

第一个，“适配器层”(Houlsby等人，2019;Bapna等人，2019)的目标是在微调时保持原始模型的大部分固定。适配器层是附加的高密度relu密集块，在Transformer的每个块中，在每个预先存在的前馈网络之后添加。这些新的前馈网络的设计使其输出维数与输入相匹配。这允许它们插入到网络中，而不需要对结构或参数进行额外的更改。在进行微调时，只更新适配器层和层标准化参数。该方法的主要超参数是前馈网络的内维数d，它改变了新参数加入模型的数量。我们用不同的值来实验d。

我们考虑的第二种微调方法是“逐渐解冻”(Howard and Ruder, 2018)。在逐渐解冻的过程中，越来越多的模型参数随时间进行微调。逐渐解冻最初应用于由**单层堆栈组成的语言模型体系结构**。在这个设置中，微调开始时只更新最后一层的参数，训练一定次数后再更新第二层的参数，以此类推，直到整个网络的参数都被微调完毕。

使这种方法适应我们的编码器-解码器模型，我们在编码器和解码器中并行地逐渐解冻层，在这两种情况下都从顶部开始。

#### Multi-task Learning

另一种方法，称为“多任务学习”(Ruder, 2017;Caruana, 1997)，是一次对模型进行多任务训练。这种方法的目标**通常是训练一个可以同时执行许多任务的单一模型，即模型及其大部分参数在所有任务中共享。**

多任务学习中一个极其重要的因素是**模型应该对每个任务中的多少数据进行训练**。我们的目标是**不让模型训练不足或过度——也就是说，我们希望模型能够从一个给定的任务中看到足够多的数据，以便它能够很好地执行任务，但又不希望看到太多的数据以至于它记住了训练集。**

##### Examples-proportional mixing

模型过拟合到给定任务的速度的一个主要因素是**任务的数据集大小**。因此，设置混合比例的一种自然方法是**按照每个任务数据集的大小进行采样。**这相当于将所有任务的数据集连接起来，并从合并的数据集中随机抽样。

##### Temperature-scaled mixing

缓解数据集大小之间巨大差异的另一种方法是**调整混合速率的“温度”**。多语言BERT使用了这种方法，以确保模型在低资源语言上得到充分的训练.为了用温度T实现温度缩放，我们**将每个任务的混合速率rm提高到1 / T的幂**，并**重新标准化速率**，使其总和为1。当T = 1时，这种方法等价于例题-比例混合，随着T的增加，比例更接近于均匀混合。

我们保留数据集大小限制K(应用于温度缩放前获取rm)，但将其设置为一个较大的值K = 221。我们使用较大的K值，因为升高温度会降低最大数据集的混合速率。

##### Equal mixing

在这种情况下，我们**以相同的概率从每个任务中抽样**。具体地说，每批中的每个例子都是从我们训练的数据集之一中随机均匀地采样的。这很可能是一种次优策略，因为模型会在低资源任务上快速过拟合，而在高资源任务上过拟合。

为了将这些混合策略与我们训练前的基线进行比较——然后对结果进行微调，我们训练了相同总步数的多任务模型:219 + 218 = 786432。结果如表11所示。

![image-20230922141303282](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230922141303282.png)

#### Combining Multi-Task Learning with Fine-Tuning

我们正在研究一种轻松版的多任务学习，我们**在混合任务中训练单个模型，但允许使用模型的不同参数设置**(检查点)来评估性能。我们可以通过考虑这样一种情况来扩展这种方法，即**在所有任务上预先训练模型，然后在单个监督任务上进行微调。**

### Scaling

机器学习研究的“惨痛教训”表明，能够利用额外计算的通用方法最终会战胜依赖人类专业知识的方法。也就是说，与更精心设计的方法相比，**按比例放大可以产生更好的性能**。然而，有多种可能的方法来进行扩展，**包括使用更大的模型、训练模型以完成更多的步骤，以及集成。**

### Putting It All Together

### Reflection

#### Takeaways

1. Text-to-text：我们的文本到文本框架提供了一种简单的方法，**使用相同的丢失函数和解码过程，在各种各样的文本任务中训练单一模型。**我们展示了这种方法如何成功地应用于**生成任务**(如抽象摘要)、**分类任务**(如自然语言推理)，甚至**回归任务**(如STS-B)。尽管文本-文本框架很简单，但我们发现它的性能可以与特定于任务的架构相媲美，并且当与规模相结合时，最终产生了最先进的结果。
2. Architectures：一些针对NLP的迁移学习工作考虑了Transformer的体系结构变体，但我们发现原**始的编码器-解码器形式**在我们的文本到文本框架中工作得最好。**我们还表明，在编码器和解码器中共享参数并不会导致性能大幅下降，同时将总参数数量减半。**
3. Unsupervised objectives：我们发现大多数“去噪”目标，即**训练模型重构随机损坏的文本**，在文本-文本设置中执行的类似。因此，我们建议**使用产生短目标序列的目标**，这样无监督前训练的计算效率更高
4. Data sets：我们介绍了“大规模清洁语料库”(C4)，它包含了从Common Crawl web转储中启发式清理的文本。当比较C4和使用额外过滤的数据集时，我们发现对域内无标记数据的训练可以提高一些下游任务的性能。
5. Training strategies：在微调过程中**更新所有预训练模型**参数的基本方法优于那些旨在更新更少参数的方法，尽管更新所有参数的代价最高。
6. Scaling：我们比较了利用额外计算的各种策略，包括在更多数据上训练模型，训练更大的模型，以及使用模型集成。我们发现，**每一种方法都能显著提高性能**，尽管在更小的模型上训练更多的数据往往比在更大的模型上训练更少的步骤效果更好。我们还表明，与单一模型相比，**模型集合可以提供更好的结果**，

### Outlook

1. The inconvenience of large models：越大的模型表现越好。用于运行这些模型的硬件越来越便宜，功能越来越强大，这一事实表明，扩展可能继续是实现更好性能的一种有前途的方式
2. More efficient knowledge extraction:**预培训的目标之一(松散地说)是为模型提供通用的“知识”，以提高其在下游任务上的表现。我们在这项工作中使用的方法，这是目前的普遍做法，是训练模型去噪的损坏范围的文本。**
3. Formalizing the similarity between tasks:对未标记域内数据进行预处理可以提高下游任务的性能
4. Language-agnostic models:在我们研究的翻译任务中，只接受英语的前培训并没有取得最先进的结果。我们还对避免需要提前指定词汇表可以编码哪些语言的后勤困难感兴趣。