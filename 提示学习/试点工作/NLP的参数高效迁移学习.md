# NLP的参数高效迁移学习

## Abstract

对大型预训练模型进行微调是一种有效的自然语言处理迁移机制。然而，在存在许多下游任务时，微调的参数效率很低:**每个任务都需要一个全新的模型**。

作为一种替代方案，我们建议使用适配器模块进行传输。适配器模块**产生一个紧凑和可扩展的模型**;他们只会为每个任务添加一些可训练的参数，并且可以在不重新查看之前的任务的情况下添加新的任务。原始网络的参数保持不变，产生了高度的参数共享。

为了证明适配器的有效性，我们将最近提出的BERT Transformer模型转移到26个不同的**文本分类**任务中，包括GLUE基准测试。适配器可获得接近最先进的性能，而每个任务只添加几个参数。在GLUE上，我们获得了0.4%的完整微调性能，每个任务只添加3.6%的参数。相比之下，微调训练每个任务100%的参数。

## Introduction

从训练前的模型转移可以在许多NLP任务中产生很强的表现(Dai & Le, 2015;Howard & Ruder, 2018年;Radford等，2018)。BERT是在无监督损耗的大型文本语料库上训练的Transformer网络，在文本分类和抽取问题回答方面达到了最先进的性能(Devlin等人，2018)。

在本文中，我们讨论了**任务以流的形式到达的在线设置**。我们的目标是**构建一个系统，在所有这些方面都表现良好，但不需要为每一个新任务训练一个全新的模型。**

任务之间的高度共享对于像云服务这样的应用程序特别有用，在这些应用程序中，需要训练模型来解决客户依次到达的许多任务。为此，我们提出了一种**迁移学习策略**，它可以**产生紧凑和可扩展的下游模型**。紧凑模型是指**使用少量额外参数解决多个任务的模型**。可扩展模型**可以逐步训练以解决新的任务**，**而不会忘记以前的任务**。

自然语言处理中最常见的两种迁移学习技术是**基于特征的迁移和微调**。我们提出了一种**基于适配器模块的替代转移方法**(Rebuffi等人，2017)。基于特征的迁移涉及到**训练前的实值嵌入向量**。这些嵌入可能是在单词(Mikolov等人，2013)、句子(Cer等人，2019)或段落水平(Le & Mikolov, 2014)。然后将嵌入的内容输入定制的下游模型。微调包括**从一个预先训练的网络中复制权重，并在下游任务中对它们进行调优**。最近的研究表明，微调通常比基于特征的迁移具有更好的性能

基于特征的传输和微调都需要**为每个任务设置一组新的权重**。如果网络的低层在任务之间共享，那么微调的参数效率更高。然而，我们提出的适配器调优方法甚至具有更高的参数效率。x轴表示每个任务训练的参数数量;这对应于解决每个额外任务所需的模型大小的边际增加。基于适配器的调优需要训练比微调少两个数量级的参数，同时获得类似的性能。

![image-20230916113954238](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916113954238.png)

适配器是**在经过训练的网络层之间添加的新模块**。基于适配器的调优与基于特性的传输和微调的区别如下

考虑一个参数为w的函数(神经网络):φw(x)。基于特征的迁移将φw与新函数χv组合，得到χv(φw(x))。**只有新的特定于任务的参数v才会被训练**。

微调包括**为每个新任务调整原始参数w**，限制紧凑性。对于适配器调优，定义了一个新函数ψw,v(x)，其中参数w是从训练前复制过来的。初始参数v0的设定使新函数近似于ψw,v0 (x)≈φw(x)。在训练中，只有**v被调优**。

对于深度网络，ψ，w,v的定义通常是在原有网的φw上添加新层。如果选择|v|<<|w|，得到的模型对于许多任务都需要~ |w|参数。由于**w是固定的**，该模型可以扩展到新的任务而不影响之前的任务。

基于适配器的调优与多任务和持续学习有关。多任务学习也产生了紧凑的模型。然而，多任务学习需要同时访问所有任务，而基于适配器的调优则不需要。适配器的不同之处在于**任务不交互，共享参数被冻结**。这意味着该模型使用少量特定于任务的参数对之前的任务具有完美的记忆。

我们在大量不同的文本分类任务中演示了适配器为NLP实现参数高效调优。关键的创新是**设计一个有效的适配器模块，并将其与基本模型集成**。我们提出了一个简单但有效的瓶颈架构。在GLUE基准测试中，我们的策略几乎与完全调优的BERT的性能相匹配，但是只使用3%的特定于任务的参数，而调优使用100%的特定于任务的参数。我们在另外17个公共文本数据集上观察到类似的结果，以及SQuAD抽取的问题回答。

## Adapter tuning for NLP

我们提出了一种策略，用于在几个下游任务上调优大型文本模型。我们的策略有三个关键属性:(i)**它获得了良好的性能**，(ii)**它允许对任务进行顺序的训练**，也就是说，它不需要同时访问所有的数据集，(iii)**它只为每个任务添加少量的额外参数。**

为了实现这些属性，我们提出了一个新的瓶颈适配器模块。使用适配器模块进行调优涉及向模型添加少量的新参数，这些参数将在下游任务中接受训练

在对深度网络进行普通微调时，对网络的顶层进行了修改。这是必需的，因为上游和下游任务的标签空间和损失不同。适配器模块执行更一般的体系结构修改，将预先训练的网络用于下游任务。具体来说，适配器调优策略涉及到向原始网络注入新层。原始网络的权值不变，而新的适配器层是随机初始化的。在标准的微调中，新的顶层和原始权值是协同训练的。相反，在自适应调优中，原始网络的参数被冻结，因此可能被许多任务共享。

适配器模块有两个主要特性:**少量参数和近似身份初始化**。与原始网络的层相比，适配器模块需要较小。这意味着当增加更多的任务时，总模型大小增长相对缓慢。自适应模型的稳定训练需要一个**近似身份初始化**,通过将适配器初始化为近似身份函数，原始网络在训练开始时不受影响。在训练期间，适配器可能会被激活，以改变整个网络中激活的分布。

### Instantiation for Transformer Networks

我们为文本转换器实例化了**基于适配器的调优**。这些模型在许多NLP任务中获得了最先进的性能，包括翻译、抽取QA和文本分类问题

图2显示了我们的适配器体系结构，以及它对Transformer的应用程序。Transformer的每一层都包含两个主要子层:注意层和前馈层。这两个层都紧接着一个投影，将特征尺寸映射回图层输入的尺寸。跨每个子层应用跳跃连接。每个子层的输出被送入层归一化。我们在每个子层后面插入两个串行适配器。适配器总是直接应用于子层的输出，在将投影返回到输入大小之后，但在将跳过连接添加回去之前。然后将适配器的输出直接传递到下面的层规范化中。

![image-20230916140557299](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916140557299.png)

为了限制参数的数量，我们提出了一个瓶颈架构。适配器首先将原始的d维特征投射到更小的维度m中，应用非线性，然后投射回d维。包括偏差在内的每层添加的参数总数为2md + d + m，通过设置m<<d，限制了每个任务添加的参数数量;在实践中，我们使用原始模型中大约0.5 - 8%的参数。瓶颈维度m提供了一种简单的方法来权衡性能和参数效率。适配器模块本身内部有一个跳过连接。使用跳过连接，如果投影层的参数初始化为接近零，则模块初始化为一个近似的恒等函数。

## Experiments

我们展示了适配器为文本任务实现参数高效传输。在GLUE基准测试中(Wang et al.， 2018)，适配器调优在完全微调BERT的0.4%以内，但它只增加了微调训练的参数数量的3%。我们在进一步的17个公共分类任务和SQuAD问题回答中证实了这一结果。分析表明，基于适配器的调优自动集中在网络的高层。

### Experimental Settings

使用公共的、预先训练的BERT transformers网络作为我们的基本模型。为了使用BERT进行分类，我们采用了Devlin等人(2018)的方法。每个序列中的**第一个标记是一个特殊的“分类标记”**。我们在这个标记的嵌入上附加一个线性层来预测类标签。

我们的培训程序也遵循Devlin et al.(2018)。我们使用Adam (Kingma & Ba, 2014)进行优化，他的学习速度在前10%的步骤中线性增长，然后线性衰减到零。所有的运行都是在4个谷歌云tpu上训练的，批处理大小为32。对于每个数据集和算法，我们运行超参数扫描，并根据验证集上的准确性选择最佳模型。对于GLUE任务，我们报告提交网站提供的测试指标。对于其他分类任务，我们报告测试集准确性。

### GLUE benchmark

我们首先对GLUE进行评估。对于这些数据集，我们从预训练的BERTLARGE模型中转移，该模型包含24层，总共有330M参数.为适配器调优执行了一个小的超参数扫描:我们在{3·10−5,3·10−4,3·10−3}中扫描学习速率，在{3,20}中扫描迭代数。我们使用固定的适配器大小(瓶颈中的单元数量)进行测试，并从{8,64,256}中为每个任务选择最佳大小。适配器大小是我们惟一调优的特定于适配器的超参数。

表1总结了结果。适配器的平均GLUE分数为80.0，而完全的微调可以达到80.4。最佳适配器大小因数据集而异。例如，为MNLI选择256，而为最小的数据集RTE选择8。总是限制到64，导致平均精度略微下降到79.6。为了求解表1中所有的数据集，微调需要9× BERT参数总数相反，适配器只需要1.3×参数。

![image-20230916142747087](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916142747087.png)

### Additional Classification Tasks

为了进一步验证适配器是否能生成紧凑、高效的模型，我们测试了额外的、公开可用的文本分类任务。这个套件包含一组不同的任务:训练示例的数量从900到330k，类的数量从2到157，平均文本长度从57到1.9k字符。

对于这些数据集，我们使用32个批处理大小。数据集是多样化的，因此我们扫描了一个广泛的学习速率范围:{1·10−5,3·10−5,1·10−4,3·10−3}。由于数据集数量庞大，我们从验证集学习曲线的检查中，手动从集合{20,50,100}中选择训练epoch的数量。

我们以{2,4,8,16,32,64}来测试适配器大小。由于某些数据集很小，对整个网络进行微调可能不是最优的。因此，我们运行一个额外的基线:可变微调。为此，我们只对最上面的n层进行微调，并冻结其余的。我们扫n∈{1,2,3,5,7,9,11,12}。在这些实验中，我们使用的是12层的BERTBASE模型，因此，当n = 12时，可变微调包含了全微调。

与GLUE任务不同的是，对于这组任务，没有一组最先进的综合数字。因此，为了确认我们基于bert的模型是有竞争力的，我们收集了我们自己的基准性能。为此，我们在标准网络拓扑上运行了大规模的超参数搜索。具体来说，我们运行单任务神经自动化算法，

该算法在前馈和卷积网络空间中搜索，这些网络堆叠在TensorFlow Hub4公开提供的预先训练的文本嵌入模块上。来自TensorFlow Hub模块的嵌入可以被冻结或微调。

表2报告了AutoML基准测试(“无BERT基线”)、微调、变量微调和适配器调优的结果。AutoML基线表明BERT模型是有竞争力的。这个基线研究了数千个模型，但是BERT模型的平均性能更好。我们看到了与GLUE相似的结果模式。适配器调优的性能接近于完全的微调(落后0.4%)。为了解决所有任务，BERTBASE需要17×参数数的微调。变量微调的性能略好于微调，同时训练的层更少。变量微调的最优设置使得每个任务平均训练52%的网络，总参数减少到9.9×。然而，适配器提供了一个更紧凑的模型。它们为每个任务引入1.14%的新参数，从而为所有17个任务带来1.19×参数。

![image-20230916143226922](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916143226922.png)

### Parameter/Performance trade-off

适配器大小控制参数的效率，较小的适配器引入的参数更少，但可能会降低性能。为了探索这种权衡，我们考虑了不同的适配器大小，并比较了两个基线:(i)**仅对BERTBASE的最顶层k层进行微调。(ii)只调优层规范化参数。**

图3显示了每个套件(GLUE和“附加的”)中所有分类任务的参数/性能权衡。在GLUE上，当更少的层被微调时，性能会急剧下降。一些额外的任务受益于训练更少的层，因此微调性能的衰减要小得多

![image-20230916143832968](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916143832968.png)

图4显示了两个GLUE任务的更多细节:MNLIm和CoLA。调优顶层将为所有k >训练更多特定于任务的参数2。当使用相当数量的特定于任务的参数进行微调时，与适配器相比，性能会大幅下降。例如，仅对顶层进行微调，就可以获得大约9M的可训练参数，在MNLIm上的验证精度为77.8%±0.1%。相比之下，尺寸为64的适配器调优产生了大约2M可训练参数和83.7%±0.1%的验证精度。相比之下，全微调在MNLIm上达到84.4%±0.02%。我们在CoLA上观察到类似的趋势。

![image-20230916144507648](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916144507648.png)

作为进一步的比较，我们单独调优了层规范化的参数。这些层只包含点式的加法和乘法，因此只引入很少的可训练参数:BERTBASE的40k。然而，这种策略的效果很差:CoLA和MNLI的性能分别下降了大约3.5%和4%。总而言之，适配器调优具有高度的参数效率，并生成具有强大性能的紧凑模型，可与全调优相媲美。训练适配器的尺寸为原始模型的0.5 - 5%，性能在BERTLARGE上公布的竞争结果的1%以内。

### SQuAD Extractive Question Answering

通过在SQuAD v1.1上运行，我们确认适配器可用于分类以外的任务(Rajpurkar等人，2018年)。给定一个问题和维基百科的段落，这个任务要求从段落中选择问题的答案。

为了进行微调，我们在{3·10−5、5·10−5、1·10−4}和{2,3,5}中扫描了训练层数、学习速率和纪元数。对于适配器，我们在{3·10−5、1·10−4、3·10−4、1·10−3}和{3,10,20}中扫描适配器大小、学习率和epoch个数。至于分类，适配器可以获得与完全微调相当的性能，而训练的参数要少得多。尺寸为64(2%参数)的适配器的最佳F1值为90.4%，而SQuAD的最佳F1值为90.7。SQuAD表现良好，即使是非常小的适配器，那些尺寸2(0.1%参数)获得一个F1的89.9。

![image-20230916145055958](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916145055958.png)

### Analysis and Discussion

我们进行消融以确定哪些适配器有影响。为此，我们删除了一些训练过的适配器，并在验证集中重新评估模型(不需要重新训练)。图6显示了从所有连续层中删除适配器时的性能变化。实验在BERTBASE上进行，适配器大小为64，在MNLI和CoLA上进行。

![image-20230916145741552](C:\Users\阿超\AppData\Roaming\Typora\typora-user-images\image-20230916145741552.png)

首先，我们发现**删除任何单一层的适配器对性能的影响很小**。热图对角线上的元素显示了从单层中移除适配器的性能，其中最大的性能下降为2%。相比之下，当所有的适配器都从网络中移除时，性能大幅下降:在MNLI上为37%，在CoLA上为69%——这是通过预测大多数类别获得的分数。这表明虽然每个适配器对整个网络的影响很小，但总体影响很大。

图6表明较低层次上的适配器的影响比较高层次上的小。从MNLI的0 - 4层移除适配器几乎不会影响性能。这表明适配器性能良好，因为它们自动优先考虑较高层。

我们研究了适配器模块对神经元数量和初始化规模的鲁棒性。在我们的主要实验中，适配器模块中的权值是从一个标准差为10−2的零均值高斯函数中提取的，该函数被截断为两个标准差。为了分析初始化规模对性能的影响，我们在区间[10−7,1]内测试了标准差。图6总结了结果。我们观察到，在两个数据集上，当标准偏差低于10−2时，适配器的性能是稳健的。然而，当初始化过大时，性能会下降，在CoLA上表现得更明显。